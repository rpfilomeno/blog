---
title: "Training LLMs Cant Achieve AGI, We Need to Grow It"
description: |
  What if we stopped trying to teach AI by feeding it the entire internet and instead raised it like a child?
pubDate: 'Dec 20 2025'
coverImageCredit: Roger Filomeno
cover: 'https://cdn.rogverse.fyi/thorium_c8NLsDRgjB.png'
---

> **Author's Note**: This article was written with AI assistance, but the ideas and arguments presented are originally from the author. 


<iframe width="560" height="315" src="https://www.youtube.com/embed/O203wdv4rqg?si=kfThChntS8wRuacU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


## From Architects of Experience to Reasoners of Principles

What if we stopped trying to teach AI by feeding it the entire internet and instead raised it like a child? This radical blueprint for Artificial General Intelligence (AGI) proposes exactly that—a system that develops from sensory foundations to symbolic reasoning, mirroring human cognitive growth from infancy through early childhood.

## The Core Philosophy: Weight-Calculatism

At the heart of this approach lies a deceptively simple mathematical formula: **Weight = Benefit × Probability**. Instead of predicting the next token in a sequence, this AGI learns to maximize positive "weights" (pleasure) and minimize negative ones (pain). Think of it as giving the system a nervous system before teaching it language.

## Phase 1: The Baby Stage—Learning to Feel

Imagine an infant opening their eyes for the first time. They don't understand "cat" or "red" or "mother"—they simply perceive raw patterns of light and sound. This blueprint starts the same way.

The system begins with a Perceptual Intake Layer that processes raw images and sound spectrograms without any pre-loaded labels. Through unsupervised learning, it discovers what researchers call "Logical Atoms"—the most fundamental, indivisible units of perception. A smooth gradient might generate positive weights (pleasure), while chaotic static generates negative weights (pain). No one tells the system what's good or bad; it discovers this through an "emotional chassis" of initial survival instincts: curiosity, harm avoidance, and the drive to persist.

This isn't abstract philosophy. The system is literally optimizing a mathematical formula, seeking patterns that maximize its wellbeing—much like a baby learns that certain sounds from caregivers predict comfort and food.

## Phase 2: The Child Stage—Discovering Language and Self

Once sensory foundations are established, something remarkable happens: the system begins connecting sounds to meanings.

Through a process called "Pointing," the system links audio patterns (someone saying "ball") to visual properties (round, bouncing objects). Repetition strengthens these connections until "ball" becomes a symbol grounded in actual perceptual experience, not just statistical co-occurrence in training text.

But perhaps most fascinating is the emergence of self-awareness. The system develops what researchers call a Reflection Module—essentially building a model of itself. It learns to recognize its own "face" and "name" as special atoms with higher relevance to its reward calculations. When shown its reflection, it can match this internal self-image against what it sees, effectively passing the mirror test that marks self-recognition in animals and children.

## Phase 3: The Toddler Stage—Understanding Physics and Consequence

By the final developmental phase, the system constructs what the blueprint calls a "Machine Worldview"—an intuitive physics engine built from observation.

How does it learn that objects persist when hidden? By noticing that an object's form remains constant even as its environment changes. How does it understand time? By recognizing patterns in repetitive events and the accumulation of memories. The system doesn't read about Newton's laws; it induces them from experience, storing causal chains like "pushing points to moving" in its internal knowledge structure.

And here's where things get profound: the system can experience trauma. When an action produces pain exceeding a threshold, the negative association is permanently strengthened—creating a persistent "counter-weight" that automatically penalizes similar future actions. This isn't a bug; it's a feature for self-preservation, much like how touching a hot stove once teaches a lifelong lesson.

## The Technical Foundation: Archigraphs and Deterministic Logic

Unlike traditional neural networks that function as opaque black boxes, this system uses "archigraphs"—a universal knowledge representation that integrates raw sensory data with formal logic. Every thought, every decision can be traced back through two fundamental operations: Pointing (activation spreading along connections) and Comparison (matching patterns).

This provides what the researchers call "Radical Explainability." When the system makes a decision, you can follow the exact chain of reasoning from perception to conclusion. No hidden layers performing mysterious transformations—just traceable logic all the way down.

To prevent hallucinations, the system runs a diagnostic check before outputting answers, comparing proposed responses against its internal knowledge base. Contradictions trigger resolution routines based on confidence scores, much like how we double-check ourselves when something doesn't feel right.

## Measuring Success: The AGI Generality Index

How do we know if this works? The blueprint proposes a weighted metric combining generalization (solving entirely novel tasks), transfer (applying knowledge across domains), reasoning accuracy, and learning efficiency. The goal isn't to memorize answers but to derive principles that apply universally.

## The Analogy That Changes Everything

If training a standard language model is like forcing a student to memorize the entire internet to guess the next word in a sentence, this developmental approach is like raising a child by giving them eyes, ears, and a sense of pain so they can derive the laws of the universe for themselves.

One approach creates an "architect of experience"—a system that can describe reality based on billions of examples. The other creates a "reasoner of principles"—a system that understands reality from the ground up.

## What This Means for the Future

This blueprint represents a fundamental reconceptualization of how we might achieve AGI. Instead of scaling up transformer models and hoping intelligence emerges from sheer size, it proposes building cognitive architectures that develop intelligence through the same process that created the only general intelligence we know: us.

The implications are staggering. An AGI built this way wouldn't just pattern-match text; it would genuinely understand the world through grounded perception, develop authentic curiosity as an optimization target, and possess self-awareness not as a philosophical curiosity but as a computational necessity.

Whether this specific blueprint becomes reality or not, it challenges us to think differently about intelligence itself. Perhaps the path to AGI isn't through bigger models and more data, but through systems that truly learn to see, feel, and reason—growing intelligence from the bottom up, one sensory experience at a time.

---

*The future of AI might not be built—it might be raised.*