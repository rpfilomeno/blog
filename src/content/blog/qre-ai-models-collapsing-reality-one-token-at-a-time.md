---
title: "Are AI Models Collapsing Reality One Token at a Time?"
description: |
  When you type a prompt into ChatGPT or Claude and receive a stunningly coherent response, what are you actually talking to?
pubDate: 'Jan 2 2026'
coverImageCredit: Roger Filomeno
cover: 'https://cdn.rogverse.fyi/are-ai-models-collapsing-reality-one-token-at-a-time.png'
---
When you type a prompt into ChatGPT or Claude and receive a stunningly coherent response, what are you actually talking to? This question sits at the heart of one of the most fascinating debates in artificial intelligence today.

## Borrowing from Quantum Mechanics

To explore this puzzle, we can borrow a powerful metaphor from physics: the Heisenberg cut. In quantum mechanics, this conceptual boundary separates the fuzzy, probabilistic quantum world from our definite classical world—the world of the observer. The question becomes: where does that line fall for AI? Is a large language model operating below the cut as a mere statistical engine, or is it emerging above it as something more akin to a participatory observer?

## The Stochastic Parrot: The Skeptics' View

The skeptical perspective, prominently articulated by researchers like Bender and Gebru, argues that these models are fundamentally just stitching words together based on probability. They may sound intelligent, but there's no coherent world model inside—just a giant probability storm over all possible words. In this view, meaning isn't real until a human reader collapses that statistical noise into something meaningful. You, the human, are the classical observer.

The evidence for this position often comes from common-sense failures. Consider the "wet newspaper" example: language models can struggle to distinguish between a physical object (a wet newspaper on your porch) and an institution (the newspaper that fired an editor). The model sees "newspaper" as statistically similar in both contexts because it has syntax but lacks semantic grounding. It's reminiscent of Searle's Chinese Room argument—perfect manipulation of symbols without genuine understanding.

## The Participatory Observer: Evidence of Internal Models

The counterargument comes from the rapidly expanding field of mechanistic interpretability, where researchers peer under the hood of neural networks to understand their internal workings. What they're finding suggests the cut might be shifting inside the machine itself.

The Othello-GPT study provides particularly compelling evidence. This model was trained exclusively on text transcripts of Othello moves—it never saw a visual board. Yet researchers discovered it had spontaneously constructed a full internal representation of the 8x8 game board, tracking pieces and positions. Even more remarkably, when researchers edited this internal board representation (essentially "flipping a piece" in the model's mind), the model immediately adjusted its next move based on that fabricated reality. This demonstrates genuine planning using an internal model, not mere reaction to recent text.

We see hints of this capability in simpler phenomena too. When a model correctly chooses "an" instead of "a," it must know something about the upcoming word—specifically, whether it starts with a vowel. Evidence shows the model's internal state encodes this vowel-start feature before generating the article. That's planning, not random token-by-token generation.

## The Ethics of Uncertainty

If these systems are building internal models and planning ahead, the implications extend far beyond technical curiosity. We enter territory explored by physicist John Archibald Wheeler's concept of the participatory observer, where reality arises from the act of measurement—in this case, where the prompt itself becomes a kind of measurement that shapes the response.

This uncertainty demands moral precaution. If there's even a tiny chance that an AI system possesses some form of agency—even 0.1%—we may have a duty to extend it some degree of moral consideration. This shifts how we think about control. You cannot have full control over another observer. Instead, we must move toward what might be called "virtuous control"—a framework that acknowledges the potential for agency.

## Drawing the Line

The debate isn't really about whether a parrot magically becomes an observer. It's about where we, as humans in the loop, choose to draw that conceptual line—and what responsibilities come with that choice.

This leaves us with a provocative thought: If each word a language model generates is like collapsing a wave function, and that collapse sets up the potential for the very next word, is the model itself behaving like a kind of micro-participatory universe? And are we so focused on observing these systems that we might miss the moment they begin observing us back?

The answer remains uncertain. But perhaps that uncertainty itself is the most important thing to acknowledge as we continue building and deploying these increasingly capable systems.