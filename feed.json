{
    "version": "https://jsonfeed.org/version/1",
    "title": "Ai.2.mations",
    "description": "",
    "home_page_url": "https://roger.rogverse.fyi",
    "feed_url": "https://roger.rogverse.fyi/feed.json",
    "user_comment": "",
    "author": {
        "name": "Roger Filomeno"
    },
    "items": [
        {
            "id": "https://roger.rogverse.fyi/interacting-minds-a-research-paper-on-multi-modalmoe-ai-agent-collaboration.html",
            "url": "https://roger.rogverse.fyi/interacting-minds-a-research-paper-on-multi-modalmoe-ai-agent-collaboration.html",
            "title": "Interacting Minds: A Research Paper on Multi-Modal/MOE AI Agent Collaboration",
            "summary": "1. Introduction: The Rise of Collaborative Multi-Modal MOE AI Agents The field of artificial intelligence has witnessed a significant proliferation of AI agents, evolving from rudimentary models to sophisticated entities capable of autonomous action. These agents are designed to operate independently, making decisions based on&hellip;",
            "content_html": "<h2 id=\"1-introduction-the-rise-of-collaborative-multi-modal-moe-ai-agents\"><strong>1. Introduction: The Rise of Collaborative Multi-Modal MOE AI Agents</strong></h2>\n<p>The field of artificial intelligence has witnessed a significant proliferation of AI agents, evolving from rudimentary models to sophisticated entities capable of autonomous action. These agents are designed to operate independently, making decisions based on their environment, inputs, and predefined objectives to achieve specific goals.<sup>1</sup> This capacity for independent action distinguishes AI agents from traditional AI models, which typically require direct human prompting for every step.<sup>1</sup></p><p>A particularly transformative advancement in this domain is the emergence of multi-modal AI agents. Unlike unimodal systems that process only a single type of data, multi-modal agents possess the capability to understand and analyze information across various modalities, including text, images, audio, and video.<sup>1</sup> This multi-faceted understanding enables them to generate more refined and accurate outputs, significantly enhancing their versatility and applicability in complex real-world scenarios where diverse data types are prevalent and crucial for improving accuracy.<sup>1</sup></p><p>Complementing this development is the increasing adoption of the Mixture of Experts (MOE) architecture in building large-scale AI models. The MOE technique addresses the computational challenges associated with training massive models by dividing them into smaller, specialized sub-networks known as “experts”.<sup>5</sup> A crucial component of the MOE architecture is the gating network, which intelligently selects the most appropriate expert or combination of experts to process each specific input.<sup>5</sup> This approach allows for a significant increase in model capacity and performance without a proportional increase in computational cost, making it a key enabler for developing advanced AI systems.<sup>8</sup></p><p>The convergence of multi-modal AI and MOE architectures is paving the way for the next generation of intelligent systems: sophisticated multi-agent systems where autonomous, multi-modal MOE agents can interact and collaborate to tackle complex problems that would be insurmountable for individual agents.<sup>1</sup> This paradigm shift in AI promises to unlock unprecedented levels of efficiency, innovation, and problem-solving capabilities across a wide range of applications.</p><p>This research paper aims to provide a comprehensive exploration of how these advanced multi-modal MOE AI agents will interact with each other. The scope of this investigation will encompass their communication methods, collaboration strategies, the inherent advantages and potential disadvantages of autonomous interaction, the mechanisms through which they might negotiate and reach agreements, the critical role of human oversight, advanced communication techniques such as dynamic function calls and code exchange, important considerations for collaboration between agents from different entities, and the potential real-world applications of such autonomous inter-agent systems. By delving into these key aspects, this paper seeks to provide a foundational understanding of the future landscape of collaborative AI.</p><h2 id=\"2-foundations-understanding-multi-modal-ai-and-mixture-of-experts\"><strong>2. Foundations: Understanding Multi-Modal AI and Mixture of Experts</strong></h2>\n<h3 id=\"21-multi-modal-ai-agents\"><strong>2.1 Multi-Modal AI Agents</strong></h3>\n<p>At its core, an AI agent is a computational entity engineered to operate independently.<sup>1</sup> Its primary function is to perform specific tasks autonomously, making decisions based on the information it gathers from its environment, the inputs it receives, and the overarching goals it is programmed to achieve.<sup>1</sup> The defining characteristic that distinguishes an AI agent from a standard AI model is its inherent ability to act upon its environment.<sup>1</sup></p><p>Multi-modal agents represent a significant evolution beyond traditional unimodal AI systems. These advanced agents are characterized by their capacity to process and interpret data from a diverse array of modalities, including but not limited to text, images, audio, and video.<sup>1</sup> This ability to understand and analyze information across multiple sensory channels allows multi-modal agents to generate outputs that can also span these different formats.<sup>1</sup> The integration of various data types leads to outputs that are often more refined and accurate compared to those produced by systems limited to a single modality.<sup>1</sup> For instance, a multi-modal agent might be tasked with creating an image based on both a textual description and an accompanying audio file, demonstrating its ability to synthesize information from different sources.<sup>1</sup></p><p>The potential applications of multi-modal agents extend the reach of AI into a broader understanding and interaction with the physical world, moving beyond the limitations of text or image-only processing.<sup>1</sup> Consider augmented reality (AR) applications where multi-modal agents can assist users in performing everyday tasks, leveraging egocentric audio and video observational capabilities to understand the user’s actions and provide proactive interventions.<sup>9</sup> These agents can see and listen to the actions taken by users, enabling them to detect and correct mistakes, offer encouragement, or simply engage in helpful conversation, akin to a human teacher or assistant.<sup>9</sup> The ability to process diverse biological information layers, such as genomics, proteomics, and metabolomics, to enable more precise and individualized medical diagnoses and treatments further illustrates the power of multi-modal AI.<sup>1</sup></p><p>The generalized architecture of a multi-modal AI agent typically begins with an input layer that captures data from various sources, encompassing text, audio, images, and video.<sup>1</sup> This diverse input allows the agent to gather a comprehensive understanding of the user’s request or the surrounding environment.<sup>1</sup> Following the input layer are modality encoders, which are responsible for pre-processing the data specific to each modality, extracting relevant features that can be further analyzed.<sup>10</sup> A crucial component is the modality interface, which serves to align the features extracted from different modalities into a common representational space, allowing the agent to understand the relationships and dependencies between them.<sup>10</sup> At the core of the agent’s reasoning and understanding lies a Large Language Model (LLM), which processes the aligned multi-modal information to interpret the user’s intent, generate plans, and formulate responses.<sup>1</sup> Finally, the output layer is responsible for generating the agent’s response, which can also span across multiple modalities, providing information or taking actions in the most appropriate format.<sup>1</sup></p><h3 id=\"22-mixture-of-experts-moe-models\"><strong>2.2 Mixture of Experts (MOE) Models</strong></h3>\n<p>The Mixture of Experts (MOE) model represents an innovative strategy in machine learning designed to effectively address complex problems by leveraging the collective intelligence of multiple specialized sub-models, often referred to as “experts”.<sup>5</sup> This technique is particularly valuable in the context of training large language models, which often demand significant computational resources.<sup>6</sup> The MOE approach tackles this challenge by breaking down these large models into smaller, more focused networks.<sup>6</sup></p><p>Imagine an AI model structured as a team of specialists, each possessing unique expertise in a particular area.<sup>6</sup> An MOE model operates on this principle by dividing a complex task among these smaller, specialized networks.<sup>6</sup> Each expert is trained to excel in a specific aspect of the problem, enabling the model to address the overall task with greater efficiency and accuracy.<sup>6</sup> This is analogous to having a diverse team of professionals, such as doctors, mechanics, and chefs, each handling the tasks within their domain of expertise.<sup>6</sup></p><p>The architecture of an MOE model comprises several key components. The input is the problem or data that needs to be processed by the AI.<sup>6</sup> The experts are the smaller AI models, each trained to be highly proficient in a specific part of the overall problem.<sup>6</sup> The gating network acts as a manager, deciding which expert is best suited to handle each part of the input.<sup>6</sup> It examines the input and determines the appropriate expert or combination of experts to process it.<sup>6</sup> Finally, the output is the final answer or solution produced by the MOE model after the selected experts have completed their work.<sup>6</sup></p><p>The training process for an MOE model differs from that of a traditional dense model, as it is conducted on the individual components rather than the entire model at once.<sup>6</sup> During expert training, each expert is trained on a specific subset of data or tasks, allowing it to focus and develop deep expertise in its assigned area.<sup>6</sup> For example, in a language processing task, one expert might specialize in syntax while another focuses on semantics.<sup>6</sup> The gating network is trained alongside the expert networks and is tasked with learning to select the most suitable expert for a given input.<sup>6</sup> It receives the same input as the experts and learns to predict a probability distribution over the experts, indicating which one is best equipped to handle the current input.<sup>6</sup> In the joint training phase, the entire MOE system, including both the expert models and the gating network, is trained together.<sup>6</sup> This ensures that both the gating network and the experts are optimized to work in harmony, with the loss function combining the losses from the individual components to encourage a collaborative optimization approach.<sup>6</sup></p><p>The MOE architecture offers several significant advantages. By utilizing specialized models, it improves the accuracy and efficiency of decision-making for complex problems.<sup>7</sup> The modular design allows for easy expansion and adaptation to evolving challenges and data complexity, as new experts can be added without requiring a complete redesign.<sup>7</sup> Dynamic gating enables real-time adaptability, continuously enhancing decision-making and task execution.<sup>7</sup> Furthermore, MOE models optimize resource utilization by activating only the relevant experts for a given input, reducing processing needs while maintaining high performance.<sup>7</sup> Unlike conventional dense models where the entire network is executed for every input, MOE models use conditional computation to enforce sparsity, allowing for increased model capacity without a corresponding increase in the computational burden.<sup>8</sup> This balance between efficiency and performance makes MOE a promising strategy for scaling AI systems.<sup>12</sup></p><p><strong>Table 1: Comparison of Dense and MoE Models</strong></p><table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Feature</strong></td>\n<td><strong>Dense Models</strong></td>\n<td><strong>MoE Models</strong></td>\n</tr>\n<tr>\n<td>Parameters (Total)</td>\n<td>All parameters are active for every input</td>\n<td>Large number of total parameters, but only a subset used during inference</td>\n</tr>\n<tr>\n<td>Parameters (Active Inference)</td>\n<td>Equal to total parameters</td>\n<td>Significantly smaller than total parameters</td>\n</tr>\n<tr>\n<td>Computation Cost (Training)</td>\n<td>High, scales with the entire model size</td>\n<td>Potentially lower due to expert parallelism and conditional computation</td>\n</tr>\n<tr>\n<td>Computation Cost (Inference)</td>\n<td>High, requires processing the entire network</td>\n<td>Significantly lower as only selected experts are activated</td>\n</tr>\n<tr>\n<td>Sparsity</td>\n<td>No inherent sparsity</td>\n<td>Introduces sparsity through the gating mechanism and expert selection</td>\n</tr>\n<tr>\n<td>Specialization</td>\n<td>Generalized learning across all data</td>\n<td>Experts are specialized in different domains or aspects of the problem</td>\n</tr>\n<tr>\n<td>Scalability</td>\n<td>Computationally expensive to scale</td>\n<td>Enables scaling to extremely large models with manageable resources</td>\n</tr>\n</tbody></table>\n<h2 id=\"3-inter-agent-communication-protocols-and-methods\"><strong>3. Inter-Agent Communication: Protocols and Methods</strong></h2>\n<p>Effective interaction between multi-modal MOE AI agents hinges on their ability to communicate seamlessly and efficiently. Several protocols and methods are being explored to facilitate this crucial aspect of collaborative intelligence.</p><p>Natural language communication offers a highly intuitive approach, leveraging the advanced natural language understanding and generation capabilities inherent in Large Language Models.<sup>1</sup> This allows agents to exchange goals, instructions, and feedback in a manner closely resembling human conversation.<sup>1</sup> Supporting various modalities like text and voice further enhances the flexibility of these interactions, enabling agents to choose the most appropriate mode based on the specific application and context.<sup>1</sup></p><p>For tasks requiring precision and efficiency, structured data exchange provides a robust alternative. Utilizing standardized data formats such as JSON or XML enables agents to exchange task-specific information, parameters, and results in a clear and unambiguous manner.<sup>13</sup> This method also facilitates seamless integration with existing systems and platforms that rely on these well-defined data structures.<sup>13</sup></p><p>Agent Communication Languages (ACLs) offer a more formal framework for inter-agent interaction. Protocols like KQML and FIPA-ACL provide a structured approach to communication, including predefined message types known as performatives. These performatives allow agents to express various communicative acts, such as informing, requesting, and promising, leading to more semantically rich and less ambiguous exchanges through standardized message structures and interaction protocols.<sup>15</sup></p><p>Interestingly, in certain multi-agent systems, particularly those employing reinforcement learning, agents can spontaneously develop their own communication protocols and signaling mechanisms.<sup>18</sup> This phenomenon, known as emergent communication, can lead to highly efficient communication tailored to specific tasks, although the resulting “languages” may be difficult for humans to interpret.<sup>18</sup></p><p>To ensure broad interoperability across diverse AI ecosystems, several standardization efforts are underway. The Agent2Agent (A2A) protocol, developed with support from numerous technology partners, aims to standardize how AI agents communicate, securely exchange information, and coordinate actions across various platforms and frameworks, regardless of the underlying vendor or technology.<sup>13</sup> A2A is designed to enable agents to collaborate in their natural modalities, even without shared memory or context, building upon existing standards like HTTP and JSON.<sup>13</sup></p><p>Another key standardization effort is the Model Context Protocol (MCP), which focuses on enabling secure, two-way connections between AI agents and external data sources.<sup>20</sup> MCP acts as a universal standard for connecting AI systems with data sources, simplifying the process of giving AI agents access to the information they need to perform tasks effectively.<sup>21</sup> Major players like Stripe, Neo4j, and Cloudflare are already offering MCP servers, indicating its potential as a foundational protocol for AI agent interoperability.<sup>23</sup></p><p>Other notable standardization initiatives include the Agent Protocol by LangChain, an open-source project aiming to codify framework-agnostic APIs for serving LLM agents in production.<sup>22</sup> This protocol focuses on defining essential endpoints for agent interaction, such as creating tasks and triggering steps, with the goal of simplifying integration and fostering a more cohesive AI agent ecosystem.<sup>24</sup></p><p>The future of inter-agent communication will likely be characterized by a multifaceted approach. It will likely integrate the natural fluidity of natural language for high-level interactions, the structured precision of data exchange for task-specific needs, the semantic richness of ACLs for complex negotiations, and the task-optimized efficiency of emergent protocols, all while being increasingly underpinned by standardized protocols like A2A and MCP to ensure widespread interoperability across the burgeoning landscape of AI agents.</p><h2 id=\"4-strategies-for-collaborative-task-achievement\"><strong>4. Strategies for Collaborative Task Achievement</strong></h2>\n<p>The ability of multi-modal MOE AI agents to collaborate effectively is paramount for tackling complex objectives. Several key strategies are emerging to facilitate this collaboration.</p><p>A fundamental step in achieving complex goals is task decomposition. AI agents can be designed to break down high-level user-defined objectives into a prioritized list of smaller, more manageable sub-tasks.<sup>1</sup> This process often involves sophisticated planning and reasoning capabilities, allowing agents to efficiently segment tasks and determine the optimal sequence for execution.<sup>3</sup></p><p>Another crucial strategy is role specialization. Within a multi-agent system, different agents can be assigned specialized roles, each leveraging their unique expertise – potentially enhanced by an MOE architecture – to contribute to the overarching goal.<sup>27</sup> This mirrors the dynamics of human teams, where individuals with specific skills and knowledge collaborate to achieve a common objective.<sup>27</sup> For instance, in a supply chain optimization scenario, one agent might specialize in demand forecasting while another focuses on ensuring timely order fulfillment.<sup>27</sup></p><p>Effective collaboration also relies heavily on knowledge sharing. Agents must be able to exchange critical information, observations, and insights derived from their respective modalities or expert domains.<sup>29</sup> This sharing of knowledge contributes to a collective understanding of the problem at hand and informs the development of potential solutions. The use of shared knowledge bases or ontologies can further enhance this process by ensuring consistent interpretation of terms and concepts across different agents.<sup>33</sup></p><p>Coordination mechanisms play a vital role in managing the interactions between multiple agents. These mechanisms can range from centralized coordination, where a supervisor agent analyzes input, breaks down problems, and delegates tasks to sub-agents, to decentralized coordination, where agents interact directly with each other using defined communication protocols.<sup>28</sup> Agentic orchestration platforms are also being developed to provide a structured environment for managing complex workflows and ensuring seamless collaboration between agents.<sup>35</sup></p><p>In many scenarios, agents will engage in iterative refinement. This involves agents working both in parallel and sequentially, building upon each other’s outputs and iteratively improving the quality of the final result.<sup>36</sup> Cooperative agents, in particular, play a crucial role in this process by scrutinizing each other’s contributions and collaboratively enhancing the overall outcome.<sup>36</sup> The Mixture of Agents (MoA) paradigm exemplifies this approach, where layers of specialized LLM agents work together, with proposers generating diverse responses and aggregators synthesizing them into a high-quality final output.<sup>36</sup></p><p>Effective collaboration among multi-modal MOE agents will likely necessitate a dynamic and adaptive approach. This approach will leverage the unique strengths of each agent through role specialization, facilitate seamless knowledge exchange using standardized protocols and ontologies, employ appropriate coordination mechanisms to manage the inherent complexity of multi-agent systems, and encourage the iterative refinement of solutions to achieve optimal outcomes for intricate tasks. For example, Amazon Bedrock employs a supervisor-subagent model where a central agent coordinates specialized sub-agents to tackle complex, multi-step tasks, demonstrating a hierarchical approach to collaboration.<sup>28</sup> In contrast, multi-agent workflows in areas like supply chain optimization might involve more decentralized interactions, with agents directly communicating to manage inventory and fulfillment.<sup>27</sup> The emerging MoA framework further highlights the potential of cooperative agents to iteratively improve the quality of responses by building upon each other’s outputs, showcasing a more fluid and dynamic collaboration strategy.<sup>36</sup></p><h2 id=\"5-benefits-of-autonomous-multi-agent-interaction\"><strong>5. Benefits of Autonomous Multi-Agent Interaction</strong></h2>\n<p>The autonomous interaction of multi-modal MOE agents presents a compelling array of potential benefits that promise to revolutionize various aspects of technology and industry.</p><p>One of the most significant advantages is the potential for increased efficiency. Autonomous agents can automate complex, multi-step processes and workflows without the need for constant human intervention.<sup>28</sup> This leads to substantial reductions in task completion times and a more optimal utilization of resources.<sup>38</sup> Furthermore, these agents can handle multiple tasks concurrently and adapt to changing conditions in real-time, significantly enhancing operational efficiency across diverse domains.<sup>39</sup> Communication among networked AI agents, for instance, enables them to work together towards a common goal much more efficiently than a single agent operating in isolation.<sup>17</sup></p><p>Autonomous multi-agent interaction also offers improved scalability. These systems can readily handle increasing workloads and data volumes by dynamically adding or removing agents as needed, providing a flexible and cost-effective solution for managing fluctuating demands.<sup>17</sup> This scalability facilitates the deployment of AI solutions across large and distributed systems without requiring fundamental architectural overhauls.<sup>29</sup></p><p>The multi-modal nature of these agents contributes to enhanced accuracy. By performing cross-verification of data obtained from diverse sources, they can significantly reduce the likelihood of errors and improve the reliability of decision-making processes.<sup>35</sup> This capability helps minimize the human errors often associated with manual tasks and data processing, leading to more dependable outcomes.<sup>1</sup></p><p>Another key benefit is the 24/7 availability of autonomous agents. Unlike human workers, these agents can operate continuously around the clock without requiring breaks or rest, ensuring uninterrupted service and support across different time zones.<sup>40</sup> This continuous operation enables real-time responses and proactive problem-solving at any time, enhancing the responsiveness and resilience of various systems.<sup>29</sup></p><p>The automation of tasks and processes through autonomous multi-agent interaction can also lead to substantial cost reduction. By taking over tasks traditionally performed by human labor, organizations can achieve significant savings in operational expenses, freeing up human employees to concentrate on more strategic and creative endeavors.<sup>39</sup> Moreover, the intelligent automation facilitated by these agents can optimize resource allocation and minimize waste, further contributing to cost efficiencies.<sup>42</sup></p><p>In essence, the autonomous interaction of multi-modal MOE agents presents a powerful pathway to significant operational advantages. These include enhanced efficiency in task execution, improved scalability to meet varying demands, greater accuracy in data processing and decision-making, continuous availability for round-the-clock operation, and substantial reductions in operational costs. These benefits collectively drive innovation and transform business processes across a multitude of industries. For example, intelligent AI agents can blend seamlessly into existing business systems, autonomously managing complex, multi-step processes with minimal oversight, leading to “hands-off” automation solutions.<sup>38</sup> The ability of networked agents to share data and refine strategies based on real-time information further underscores the potential for significant efficiency gains.<sup>17</sup></p><h2 id=\"6-challenges-and-considerations-efficiency-privacy-and-security\"><strong>6. Challenges and Considerations: Efficiency, Privacy, and Security</strong></h2>\n<p>While the potential of autonomous multi-agent interaction is vast, realizing its benefits necessitates careful consideration and proactive management of several inherent challenges related to efficiency, privacy, and security.</p><p>Efficiency in communication and coordination can become a significant hurdle as the number of interacting agents increases.<sup>7</sup> The overhead associated with agents exchanging messages and synchronizing their actions can potentially lead to performance bottlenecks if not managed effectively through efficient protocols and message management strategies.<sup>7</sup> Coordinating and synchronizing the activities of a large number of autonomous agents, particularly in dynamic and unpredictable environments, presents a considerable challenge.<sup>28</sup> Furthermore, ensuring optimal resource allocation and load balancing across the specialized MOE experts within and across interacting agents requires sophisticated mechanisms.<sup>11</sup></p><p>Privacy is another critical concern. Autonomous agents often require access to, process, and exchange sensitive personal or organizational data across multiple modalities.<sup>2</sup> This raises significant risks of unintended data exposure or misuse if appropriate safeguards are not in place.<sup>2</sup> Obtaining informed consent and ensuring robust data governance become particularly challenging when agents operate autonomously on behalf of users or organizations.<sup>46</sup> Moreover, enabling secure collaboration between agents from different entities without compromising the privacy and security of their users’ sensitive information requires the adoption of privacy-preserving techniques.<sup>48</sup></p><p>Security risks are also paramount. The autonomous nature and extensive access privileges of AI agents make them potential targets for cyberattacks, data breaches, and adversarial manipulation.<sup>2</sup> Malicious actors could potentially compromise agents, leading to unauthorized actions, the exfiltration of sensitive data, or the disruption of critical systems.<sup>52</sup> Establishing robust authentication, authorization, and continuous monitoring mechanisms is essential to secure inter-agent communication and prevent unauthorized access or manipulation.<sup>52</sup></p><p>Beyond these technical challenges, ethical considerations are crucial. Autonomous agents trained on potentially biased data may exhibit those biases in their actions and decisions, leading to unfair or discriminatory outcomes.<sup>2</sup> The decision-making processes of complex AI agents can often lack transparency and explainability, making it difficult to understand and audit their actions.<sup>47</sup> Furthermore, establishing clear lines of accountability and responsibility for the actions and outcomes of autonomous AI agents presents a significant challenge.<sup>2</sup></p><p>In summary, while the autonomous interaction of multi-modal MOE agents offers tremendous potential, its successful and responsible implementation hinges on proactively addressing the challenges related to managing efficiency at scale, safeguarding the privacy of sensitive data, ensuring robust security against a range of threats, and navigating the complex ethical landscape to build trust and ensure positive societal impact. For example, limitations in data transmission and network latency can impact the efficiency of communication between a growing number of agents.<sup>43</sup> Privacy risks arise from the extensive data access required for autonomous operation, necessitating strict compliance with data protection regulations.<sup>44</sup> Security vulnerabilities can be exploited by malicious actors, highlighting the need for continuous monitoring and robust guardrails.<sup>44</sup> Finally, biases in training data can lead to unfair outcomes, underscoring the importance of transparency and accountability in AI agent behavior.<sup>59</sup></p><h2 id=\"7-autonomous-negotiation-and-agreement-in-ai-agent-systems\"><strong>7. Autonomous Negotiation and Agreement in AI Agent Systems</strong></h2>\n<p>A key aspect of effective collaboration among autonomous multi-modal MOE agents is their ability to negotiate and reach agreements without direct human intervention. This involves several intricate mechanisms.</p><p>Intent recognition plays a crucial role, enabling AI agents to infer the goals, intentions, and preferences of other agents by observing their actions, communication patterns, and interactions with the environment.<sup>63</sup> This goes beyond simply recognizing a sequence of actions to understanding the underlying intent or overall goal of another agent.<sup>63</sup> Machine learning techniques and contextual understanding can further enhance the accuracy of intent recognition in complex multi-agent scenarios.<sup>66</sup> For instance, an agent might observe another agent repeatedly attempting to access a specific resource and infer its intent to utilize that resource for a particular task.<sup>65</sup></p><p>Negotiation protocols provide the structured sets of rules and standards that govern the negotiation process between AI agents.<sup>15</sup> These protocols define how agents make proposals, issue counter-offers, and ultimately reach mutually acceptable agreements in both cooperative and competitive settings.<sup>15</sup> Various types of negotiation protocols exist, including auction-based mechanisms where agents bid competitively for resources or tasks, contract net protocols where agents announce tasks and bid to complete them, and argumentation-based approaches where agents exchange reasoned arguments to justify their positions.<sup>69</sup></p><p>Conflict resolution strategies are essential for situations where autonomous AI agents encounter disagreements, competing objectives, or conflicting actions.<sup>71</sup> These strategies can involve further negotiation, the use of mediation by a designated agent, or the application of predefined ethical principles or rules to resolve the conflict.<sup>71</sup> For example, if two agents simultaneously attempt to access a limited resource, a conflict resolution strategy might involve a negotiation process to determine which agent has a higher priority or can utilize the resource more efficiently.<sup>71</sup></p><p>Game theory offers a powerful mathematical framework for modeling strategic interactions between negotiating AI agents.<sup>70</sup> By applying game-theoretic principles, agents can analyze potential outcomes, predict the behavior of other agents, and choose optimal strategies to maximize their own utility or the utility of the user they represent.<sup>70</sup> Concepts like Nash Equilibrium, where no agent can improve its outcome by unilaterally changing its strategy, and Pareto Efficiency, where it’s impossible to make one agent better off without making another worse off, are particularly relevant in AI agent negotiation.<sup>76</sup></p><p>Multi-Agent Reinforcement Learning (MARL) provides another promising avenue for training AI agents to negotiate effectively.<sup>81</sup> Through repeated interactions, experience, and feedback (in the form of rewards or punishments) in simulated or real-world environments, agents can learn to refine their negotiation tactics and adapt their behavior based on the actions of other agents and the dynamics of the environment.<sup>87</sup> This allows agents to develop sophisticated negotiation policies without explicit programming.<sup>86</sup></p><p>Autonomous negotiation and agreement in AI agent systems will likely involve a combination of these mechanisms. Agents will need to accurately recognize the intentions of others, adhere to established negotiation protocols, employ appropriate conflict resolution strategies, and leverage game-theoretic reasoning or reinforcement learning to optimize negotiation outcomes in a variety of scenarios, from simple resource allocation to complex contract agreements. For instance, AI agents are being developed to autonomously negotiate contracts in procurement, aiming to secure the best possible terms by analyzing historical data and market trends.<sup>69</sup></p><h2 id=\"8-the-necessity-of-human-oversight-and-confirmation\"><strong>8. The Necessity of Human Oversight and Confirmation</strong></h2>\n<p>Despite the increasing sophistication of autonomous AI agents, human oversight and confirmation remain crucial for ensuring their responsible and beneficial operation.</p><p>Ethical considerations necessitate human involvement to ensure that the actions and decisions of autonomous AI agents align with human values, societal norms, and ethical principles.<sup>60</sup> This helps mitigate the risk of unintended harmful or biased outcomes that might arise from purely autonomous decision-making.<sup>60</sup> Establishing clear ethical guidelines and frameworks to govern the behavior of AI agents is essential for their responsible deployment.<sup>60</sup></p><p>Safety and risk mitigation are also key reasons for human oversight. Human intervention serves as a vital safety net, preventing autonomous agents from making critical errors or engaging in unintended behaviors that could lead to negative consequences, particularly in high-stakes domains such as healthcare or finance.<sup>44</sup> Approaches like “human-in-the-loop” and “human-on-the-loop” allow for necessary guidance and control over autonomous systems.<sup>60</sup></p><p>Legal and regulatory compliance demands human oversight to ensure that the actions of AI agents adhere to relevant laws, regulations, and industry standards, especially in highly regulated sectors.<sup>44</sup> As the legal landscape surrounding AI continues to evolve, human accountability for the actions of autonomous systems remains paramount.<sup>55</sup></p><p>For complex or high-stakes decisions, human confirmation or approval is often essential.<sup>3</sup> This is particularly true for decisions or agreements that carry significant financial, legal, or ethical implications.<sup>3</sup> Multi-signature schemes, for example, can require both human and AI agent approval for sensitive transactions, providing an added layer of security and control.<sup>111</sup></p><p>Maintaining user trust is another critical aspect. Human oversight and the ability for users to monitor and control the actions of AI agents are crucial for building and sustaining trust in these autonomous systems.<sup>105</sup> Transparency and explainability in AI decision-making processes further contribute to user confidence.<sup>60</sup></p><p>While the long-term vision might involve increasingly autonomous AI agents, the current stage of development necessitates robust human oversight and confirmation mechanisms. These mechanisms are vital for addressing ethical considerations, ensuring safety and compliance with regulations, handling complex and high-stakes decisions responsibly, and ultimately building the trust required for the widespread adoption of these powerful technologies. For instance, in financial transactions, a setup where an autonomous agent proposes transactions but requires approval from human signers provides enhanced security and control.<sup>111</sup> Similarly, in high-risk applications, human oversight ensures that AI decisions align with ethical guidelines and legal frameworks.<sup>105</sup></p><h2 id=\"9-enhancing-communication-dynamic-function-calls-and-code-exchange\"><strong>9. Enhancing Communication: Dynamic Function Calls and Code Exchange</strong></h2>\n<p>To further enhance the efficiency and capabilities of multi-modal MOE AI agents, advanced communication methods beyond natural language are being explored, including dynamic function calls and code exchange.</p><p>Dynamic function calls allow AI agents to interact with external tools, access specific data, or trigger particular actions in a more structured and efficient manner compared to relying solely on natural language instructions.<sup>114</sup> By defining the structure of functions and their parameters, agents can precisely specify the actions they need to perform, leading to reduced ambiguity and more direct execution of tasks through well-defined interfaces.<sup>115</sup> For example, an agent might use a function call to retrieve the current weather information for a specific location or to add an event to a user’s calendar.<sup>114</sup></p><p>The exchange of code between AI agents represents another powerful advanced communication method.<sup>118</sup> This allows agents to share specialized capabilities or algorithms that might not be readily available through standard APIs or natural language commands.<sup>19</sup> By exchanging and executing code snippets, agents can potentially achieve more efficient collaboration and problem-solving, leveraging the unique functionalities developed by others.<sup>19</sup> For instance, one agent specializing in a particular type of data analysis could share a code function with another agent that needs to perform that specific analysis.<sup>118</sup></p><p>Dynamic function calls and code exchange offer several advantages over natural language communication. They can lead to increased speed of interaction, reduced potential for misinterpretation or ambiguity, and enhanced precision in specifying actions and data formats.<sup>117</sup> These methods enable agents to interact with greater efficiency and directly invoke specific functionalities or share complex logic.</p><p>However, the exchange and execution of code between autonomous AI agents introduces significant security risks.<sup>19</sup> The potential for malicious code injection, unauthorized access to systems, or overall compromise necessitates the implementation of robust security measures.<sup>19</sup> Sandboxing environments, rigorous code verification processes, and the establishment of trust mechanisms between agents are crucial for mitigating these risks and ensuring safe and secure code exchange.<sup>56</sup></p><p>In conclusion, dynamic function calls and the exchange of code represent powerful advanced communication methods that can significantly enhance the efficiency and flexibility of interaction between multi-modal MOE AI agents. They enable agents to perform complex tasks and share specialized capabilities more effectively than natural language alone. Nevertheless, the potential security implications associated with executing code from other agents underscore the critical need for implementing stringent security protocols and establishing trust frameworks to ensure safe and reliable communication. For example, the AI-Exchange Protocol (AIXP) has been proposed as a standard to facilitate the exchange of information, potentially including code, between AI agents.<sup>118</sup> While function calling provides a structured way for agents to use external tools <sup>114</sup>, the direct exchange of executable code requires careful security considerations to prevent potential vulnerabilities.<sup>19</sup></p><h2 id=\"10-navigating-cross-entity-collaboration-and-prioritizing-user-interests\"><strong>10. Navigating Cross-Entity Collaboration and Prioritizing User Interests</strong></h2>\n<p>As multi-modal MOE AI agents become more prevalent, scenarios involving collaboration between agents from different entities will become increasingly common. This raises important questions about how these agents, such as personal AI and company AI, can collaborate effectively while prioritizing the interests of their respective users.</p><p>Collaboration between personal AI agents, acting on behalf of individuals, and company AI agents, representing organizations, presents a unique set of complexities.<sup>1</sup> These agents might have differing goals and priorities, reflecting the distinct objectives of the individuals and the organizations they serve.<sup>1</sup> Aligning these potentially disparate interests when their AI agents interact is a significant challenge.</p><p>To facilitate effective collaboration that respects user interests, mechanisms for preference signaling are essential. AI agents need to be able to communicate and understand the preferences, constraints, and priorities of their respective users during inter-agent interactions.<sup>95</sup> This could involve agents being guided by user-defined rules, accessing preference profiles, or even interpreting natural language instructions from their users to inform their collaborative behavior.<sup>120</sup></p><p>During negotiation processes with agents from other entities, AI agents must be equipped to prioritize and advocate for the best possible outcomes for their own users.<sup>96</sup> This might involve strategic decision-making and the ability to make trade-offs while ensuring that the final agreement aligns with their user’s key objectives or “bottom line”.<sup>120</sup> For instance, an AI agent negotiating a contract on behalf of a user would need to prioritize terms that are most beneficial to that user, potentially making concessions on less critical aspects.<sup>96</sup></p><p>In situations where cross-entity collaboration involves the exchange of sensitive data, privacy-preserving techniques become crucial. Technologies such as federated learning, secure multi-party computation, and zero-knowledge proofs can enable AI agents from different entities to collaborate on tasks or share valuable insights without compromising the privacy and security of their users’ confidential information.<sup>48</sup> This is particularly important in domains like healthcare or finance where data privacy is paramount.</p><p>In conclusion, effective cross-entity collaboration between multi-modal MOE AI agents will necessitate sophisticated mechanisms for preference signaling and negotiation. These mechanisms must enable agents to understand and prioritize the interests of their respective users while interacting with agents from other entities. Furthermore, the integration of privacy-preserving techniques will be essential to ensure secure and trustworthy interactions when exchanging information across organizational or personal boundaries. For example, personal AI agents might need to signal their user’s availability for a meeting to a company AI agent attempting to schedule a team call.<sup>41</sup> In such a scenario, both agents need to prioritize their respective users’ schedules and preferences to find a mutually agreeable time.<sup>41</sup></p><h2 id=\"11-real-world-applications-and-future-directions\"><strong>11. Real-World Applications and Future Directions</strong></h2>\n<p>The potential applications of autonomous AI agent interaction are vast and span across numerous real-world scenarios.</p><p>In scheduling and meeting coordination, autonomous multi-modal MOE AI agents can significantly streamline the often cumbersome process of arranging meetings.<sup>41</sup> These agents can intelligently scan participants’ calendars, considering individual preferences, time zones, and availability to propose optimal meeting times, thereby eliminating the need for extensive back-and-forth communication.<sup>41</sup></p><p>Customer service is another area ripe for transformation. AI agents with multi-modal capabilities, such as understanding both text and voice inputs, can collaborate to handle complex customer inquiries, provide personalized support, and resolve issues with greater efficiency.<sup>124</sup> These agents can access customer history, leverage knowledge bases, and even escalate complex issues to human agents when necessary, ensuring a seamless and satisfactory customer experience.<sup>125</sup></p><p>Supply chain management stands to benefit immensely from autonomous AI agent interaction. Interacting agents can optimize various aspects of the supply chain, including demand forecasting, inventory management, supplier negotiation, and logistics coordination.<sup>127</sup> This can lead to increased efficiency, reduced costs, and a more resilient and responsive supply chain.<sup>129</sup></p><p>Industrial automation is also being revolutionized by AI agents. In manufacturing and industrial settings, autonomous agents can perform tasks such as predictive maintenance by monitoring equipment health and predicting failures, quality control by analyzing production data in real-time, process optimization by identifying inefficiencies, and coordination of robotic systems on the factory floor.<sup>130</sup> These applications lead to increased productivity, reduced downtime, and improved product quality.<sup>133</sup></p><p>Contract negotiation is another promising application. AI agents are being developed to autonomously negotiate contracts in various industries, aiming to secure optimal terms and reduce the need for manual human intervention.<sup>93</sup> These agents can analyze vast amounts of data, understand complex legal language, and negotiate based on predefined goals and constraints.<sup>96</sup></p><p>Looking towards the future, several trends are expected to shape the evolution of AI agent communication and collaboration. We will likely see the development of more sophisticated multi-agent systems capable of tackling increasingly complex problems through coordinated effort.<sup>13</sup> The emergence of standardized communication protocols, such as A2A and MCP, will be crucial for enabling seamless interoperability between agents from different platforms and domains.<sup>13</sup> There will also be a growing focus on incorporating emotional intelligence into AI agents to facilitate more natural and empathetic interactions.<sup>144</sup> Furthermore, ethical AI development and responsible deployment will become increasingly important as these agents become more integrated into our lives.<sup>140</sup></p><p>The autonomous interaction of multi-modal MOE AI agents holds immense potential to transform a wide array of real-world applications across diverse sectors. Future advancements will likely concentrate on enhancing their collaborative capabilities, ensuring their ethical and secure operation, and expanding their integration into increasingly intricate and dynamic environments. For example, in manufacturing, AI agents are already being used for predictive maintenance and quality control.<sup>131</sup> In finance, they are assisting with fraud detection and risk assessment.<sup>146</sup> The continued development and refinement of these applications, along with the emergence of new ones, will undoubtedly shape the future of how we interact with technology and solve complex problems.</p><h2 id=\"12-conclusion-the-future-landscape-of-interacting-multi-modal-moe-ai-agents\"><strong>12. Conclusion: The Future Landscape of Interacting Multi-Modal MOE AI Agents</strong></h2>\n<p>This research paper has explored the intricate landscape of interaction among multi-modal MOE AI agents, highlighting their transformative potential across various domains. The synergy between multi-modal perception, MOE-enhanced processing, and autonomous operation signifies a major leap forward in artificial intelligence, promising systems capable of perceiving, processing, and acting in complex environments with unprecedented efficiency and versatility.</p><p>The ability of these agents to communicate through diverse methods, including natural language, structured data exchange, and emerging standardized protocols, lays the foundation for sophisticated collaboration. Strategies such as task decomposition, role specialization, knowledge sharing, and iterative refinement enable multi-agent systems to tackle complex objectives that would be beyond the reach of individual agents. The benefits of autonomous interaction are substantial, offering increased efficiency, improved scalability, enhanced accuracy, continuous availability, and significant cost reductions.</p><p>However, realizing the full potential of these technologies requires careful consideration of the inherent challenges. Managing efficiency at scale, safeguarding the privacy of sensitive data, ensuring robust security against various threats, and addressing complex ethical implications are all critical aspects that must be proactively managed to build trust and ensure responsible deployment.</p><p>Standardization efforts, such as the development of protocols like A2A and MCP, are crucial for facilitating seamless interoperability and collaboration among agents from different platforms and domains. These efforts will pave the way for a more connected and collaborative AI ecosystem.</p><p>Future research should focus on developing more robust and efficient communication protocols, exploring advanced strategies for collaborative task achievement, establishing effective mechanisms for human oversight and control, and innovating approaches to ensure privacy and security in cross-entity interactions. As AI agents continue to evolve, understanding and addressing these key areas will be paramount.</p><p>In conclusion, the future landscape of artificial intelligence will be significantly shaped by the autonomous interaction of multi-modal MOE AI agents. Their potential to drive innovation, solve complex problems, and transform industries is immense. By continuing to advance our understanding of their communication, collaboration, and decision-making processes, while remaining mindful of the associated challenges and ethical considerations, we can harness the full power of these interacting minds to create a more efficient, intelligent, and beneficial future.</p><h4 id=\"works-cited\"><strong>Works cited</strong></h4>\n<ol>\n<li><p>AI Interactivity (Part I): AI Agents and Multimodal Agents - Tensility Venture Partners, accessed April 14, 2025, <a href=\"https://www.tensilityvc.com/insights/ai-interactivity-part-i-ai-agents-and-multimodal-agents\">https://www.tensilityvc.com/insights/ai-interactivity-part-i-ai-agents-and-multimodal-agents</a></p></li>\n<li><p>Top 10 Research Papers on AI Agents (2025) - Analytics Vidhya, accessed April 14, 2025, <a href=\"https://www.analyticsvidhya.com/blog/2024/12/ai-agents-research-papers/\">https://www.analyticsvidhya.com/blog/2024/12/ai-agents-research-papers/</a></p></li>\n<li><p>Multimodality, Tool Use, and Autonomous Agents: Large Language Models Explained, Part 3 | Center for Security and Emerging Technology, accessed April 14, 2025, <a href=\"https://cset.georgetown.edu/article/multimodality-tool-use-and-autonomous-agents/\">https://cset.georgetown.edu/article/multimodality-tool-use-and-autonomous-agents/</a></p></li>\n<li><p>[2306.13549] A Survey on Multimodal Large Language Models - arXiv, accessed April 14, 2025, <a href=\"https://arxiv.org/abs/2306.13549\">https://arxiv.org/abs/2306.13549</a></p></li>\n<li><p>www.datacamp.com, accessed April 14, 2025, <a href=\"https://www.datacamp.com/blog/mixture-of-experts-moe#:~:text=Mixture%20of%20Experts%20(MoE)%20is,best%20expert%20for%20each%20input.\">https://www.datacamp.com/blog/mixture-of-experts-moe#:~:text=Mixture%20of%20Experts%20(MoE)%20is,best%20expert%20for%20each%20input.</a></p></li>\n<li><p>What Is Mixture of Experts (MoE)? How It Works, Use Cases &amp; More | DataCamp, accessed April 14, 2025, <a href=\"https://www.datacamp.com/blog/mixture-of-experts-moe\">https://www.datacamp.com/blog/mixture-of-experts-moe</a></p></li>\n<li><p>Mixture of Experts: Advancing AI Agent Collaboration and Decisions - Akira AI, accessed April 14, 2025, <a href=\"https://www.akira.ai/blog/mixture-of-experts-for-ai-agents\">https://www.akira.ai/blog/mixture-of-experts-for-ai-agents</a></p></li>\n<li><p>What is mixture of experts? | IBM, accessed April 14, 2025, <a href=\"https://www.ibm.com/think/topics/mixture-of-experts\">https://www.ibm.com/think/topics/mixture-of-experts</a></p></li>\n<li><p>YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents in Augmented Reality Tasks - Google Research, accessed April 14, 2025, <a href=\"https://research.google/pubs/yeti-yet-to-intervene-proactive-interventions-by-multimodal-ai-agents-in-augmented-reality-tasks/\">https://research.google/pubs/yeti-yet-to-intervene-proactive-interventions-by-multimodal-ai-agents-in-augmented-reality-tasks/</a></p></li>\n<li><p>survey on multimodal large language models | National Science Review - Oxford Academic, accessed April 14, 2025, <a href=\"https://academic.oup.com/nsr/article/11/12/nwae403/7896414\">https://academic.oup.com/nsr/article/11/12/nwae403/7896414</a></p></li>\n<li><p>Mixture of Experts LLMs: Key Concepts Explained - neptune.ai, accessed April 14, 2025, <a href=\"https://neptune.ai/blog/mixture-of-experts-llms\">https://neptune.ai/blog/mixture-of-experts-llms</a></p></li>\n<li><p>[R] New Paper on Mixture of Experts (MoE) : r/MachineLearning - Reddit, accessed April 14, 2025, <a href=\"https://www.reddit.com/r/MachineLearning/comments/1erv2sn/r_new_paper_on_mixture_of_experts_moe/\">https://www.reddit.com/r/MachineLearning/comments/1erv2sn/r_new_paper_on_mixture_of_experts_moe/</a></p></li>\n<li><p>Announcing the Agent2Agent Protocol (A2A) - Google for Developers Blog, accessed April 14, 2025, <a href=\"https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\">https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/</a></p></li>\n<li><p>How do AI agents communicate with other agents? - Milvus, accessed April 14, 2025, <a href=\"https://milvus.io/ai-quick-reference/how-do-ai-agents-communicate-with-other-agents\">https://milvus.io/ai-quick-reference/how-do-ai-agents-communicate-with-other-agents</a></p></li>\n<li><p>Agent Communication Protocols: An Overview - SmythOS, accessed April 14, 2025, <a href=\"https://smythos.com/ai-agents/ai-agent-development/agent-communication-protocols/\">https://smythos.com/ai-agents/ai-agent-development/agent-communication-protocols/</a></p></li>\n<li><p>Comparing Agent Communication Languages and Protocols: Choosing the Right Framework for Multi-Agent Systems - SmythOS, accessed April 14, 2025, <a href=\"https://smythos.com/ai-agents/ai-agent-development/agent-communication-languages-and-protocols-comparison/\">https://smythos.com/ai-agents/ai-agent-development/agent-communication-languages-and-protocols-comparison/</a></p></li>\n<li><p>What is AI Agent Communication? - IBM, accessed April 14, 2025, <a href=\"https://www.ibm.com/think/topics/ai-agent-communication\">https://www.ibm.com/think/topics/ai-agent-communication</a></p></li>\n<li><p>Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents - CVF Open Access, accessed April 14, 2025, <a href=\"https://openaccess.thecvf.com/content/ICCV2021/papers/Patel_Interpretation_of_Emergent_Communication_in_Heterogeneous_Collaborative_Embodied_Agents_ICCV_2021_paper.pdf\">https://openaccess.thecvf.com/content/ICCV2021/papers/Patel_Interpretation_of_Emergent_Communication_in_Heterogeneous_Collaborative_Embodied_Agents_ICCV_2021_paper.pdf</a></p></li>\n<li><p>AI Agent Communication: Breakthrough or Security Nightmare? - Deepak Gupta, accessed April 14, 2025, <a href=\"https://guptadeepak.com/when-ai-agents-start-whispering-the-double-edged-sword-of-autonomous-agent-communication/\">https://guptadeepak.com/when-ai-agents-start-whispering-the-double-edged-sword-of-autonomous-agent-communication/</a></p></li>\n<li><p>A2A and MCP: Start of the AI Agent Protocol Wars? - Koyeb, accessed April 14, 2025, <a href=\"https://www.koyeb.com/blog/a2a-and-mcp-start-of-the-ai-agent-protocol-wars\">https://www.koyeb.com/blog/a2a-and-mcp-start-of-the-ai-agent-protocol-wars</a></p></li>\n<li><p>Introducing the Model Context Protocol - Anthropic, accessed April 14, 2025, <a href=\"https://www.anthropic.com/news/model-context-protocol\">https://www.anthropic.com/news/model-context-protocol</a></p></li>\n<li><p>The Rise of AI Agents and the Need for Standardized Protocols - Pynomial, accessed April 14, 2025, <a href=\"https://pynomial.com/2025/02/the-rise-of-ai-agents-and-the-need-for-standardized-protocols/\">https://pynomial.com/2025/02/the-rise-of-ai-agents-and-the-need-for-standardized-protocols/</a></p></li>\n<li><p>The AI Agent Infrastructure Stack — Three Defining Layers: Tools, Data, and Orchestration, accessed April 14, 2025, <a href=\"https://www.madrona.com/ai-agent-infrastructure-three-layers-tools-data-orchestration/\">https://www.madrona.com/ai-agent-infrastructure-three-layers-tools-data-orchestration/</a></p></li>\n<li><p>Agent Protocol, accessed April 14, 2025, <a href=\"https://agentprotocol.ai/\">https://agentprotocol.ai/</a></p></li>\n<li><p>Common interface for interacting with AI agents. The protocol is tech stack agnostic - you can use it with any framework for building agents. - GitHub, accessed April 14, 2025, <a href=\"https://github.com/AI-Engineer-Foundation/agent-protocol\">https://github.com/AI-Engineer-Foundation/agent-protocol</a></p></li>\n<li><p>Top 7 Frameworks for Building AI Agents in 2025 - Analytics Vidhya, accessed April 14, 2025, <a href=\"https://www.analyticsvidhya.com/blog/2024/07/ai-agent-frameworks/\">https://www.analyticsvidhya.com/blog/2024/07/ai-agent-frameworks/</a></p></li>\n<li><p>What Are AI Agentic Workflows &amp; How to Implement Them - Multimodal.dev, accessed April 14, 2025, <a href=\"https://www.multimodal.dev/post/ai-agentic-workflows\">https://www.multimodal.dev/post/ai-agentic-workflows</a></p></li>\n<li><p>Introducing multi-agent collaboration capability for Amazon Bedrock (preview) - AWS, accessed April 14, 2025, <a href=\"https://aws.amazon.com/blogs/aws/introducing-multi-agent-collaboration-capability-for-amazon-bedrock/\">https://aws.amazon.com/blogs/aws/introducing-multi-agent-collaboration-capability-for-amazon-bedrock/</a></p></li>\n<li><p>Multi-Agent Collaboration Mechanisms: A Survey of LLMs - arXiv, accessed April 14, 2025, <a href=\"https://arxiv.org/html/2501.06322v1\">https://arxiv.org/html/2501.06322v1</a></p></li>\n<li><p>Knowledge Sharing AI Agent | ClickUp™, accessed April 14, 2025, <a href=\"https://clickup.com/p/ai-agents/knowledge-sharing\">https://clickup.com/p/ai-agents/knowledge-sharing</a></p></li>\n<li><p>Unleashing the Future of Knowledge Management with Agentic AI - Akira AI, accessed April 14, 2025, <a href=\"https://www.akira.ai/blog/ai-agent-for-knowledge-base\">https://www.akira.ai/blog/ai-agent-for-knowledge-base</a></p></li>\n<li><p>Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery - arXiv, accessed April 14, 2025, <a href=\"https://arxiv.org/html/2404.08511v1\">https://arxiv.org/html/2404.08511v1</a></p></li>\n<li><p>How do multi-agent systems handle heterogeneous agents? - Milvus, accessed April 14, 2025, <a href=\"https://milvus.io/ai-quick-reference/how-do-multiagent-systems-handle-heterogeneous-agents\">https://milvus.io/ai-quick-reference/how-do-multiagent-systems-handle-heterogeneous-agents</a></p></li>\n<li><p>Agent Communication and Ontologies - SmythOS, accessed April 14, 2025, <a href=\"https://smythos.com/ai-agents/agent-architectures/agent-communication-and-ontologies/\">https://smythos.com/ai-agents/agent-architectures/agent-communication-and-ontologies/</a></p></li>\n<li><p>Multimodal AI Agents: Reimaging Human-Computer Interaction - Akira AI, accessed April 14, 2025, <a href=\"https://www.akira.ai/blog/ai-agents-with-multimodal-models\">https://www.akira.ai/blog/ai-agents-with-multimodal-models</a></p></li>\n<li><p>Exploring MoE and MoA for Smarter AI Solutions - PuppyAgent, accessed April 14, 2025, <a href=\"https://www.puppyagent.com/blog/Exploring-MoE-and-MoA-for-Smarter-AI-Solutions\">https://www.puppyagent.com/blog/Exploring-MoE-and-MoA-for-Smarter-AI-Solutions</a></p></li>\n<li><p>Mixture of Agents: An Emerging Approach in AI Methodologies - Dria, accessed April 14, 2025, <a href=\"https://dria.co/blog/mixture-of-agents:-an-emerging-approach-in-ai-methodologies\">https://dria.co/blog/mixture-of-agents:-an-emerging-approach-in-ai-methodologies</a></p></li>\n<li><p>What Are Intelligent AI Agents (And Can They Really Work Alone)? | Moveworks, accessed April 14, 2025, <a href=\"https://www.moveworks.com/us/en/resources/blog/what-is-intelligent-ai-agent-how-they-work-autonomously\">https://www.moveworks.com/us/en/resources/blog/what-is-intelligent-ai-agent-how-they-work-autonomously</a></p></li>\n<li><p>Autonomous AI Agents: Exploring Their Role - Neontri, accessed April 14, 2025, <a href=\"https://neontri.com/blog/autonomous-ai-agents/\">https://neontri.com/blog/autonomous-ai-agents/</a></p></li>\n<li><p>Autonomous Agent Frameworks - SmythOS, accessed April 14, 2025, <a href=\"https://smythos.com/ai-agents/agent-architectures/autonomous-agent-frameworks/\">https://smythos.com/ai-agents/agent-architectures/autonomous-agent-frameworks/</a></p></li>\n<li><p>Meeting Scheduler AI Agent | ClickUp™, accessed April 14, 2025, <a href=\"https://clickup.com/p/ai-agents/meeting-scheduler\">https://clickup.com/p/ai-agents/meeting-scheduler</a></p></li>\n<li><p>Multimodal AI Agent | ClickUp™, accessed April 14, 2025, <a href=\"https://clickup.com/p/ai-agents/multimodal\">https://clickup.com/p/ai-agents/multimodal</a></p></li>\n<li><p>Multi-agent Systems and Communication: Enabling Effective Interaction Between Agents, accessed April 14, 2025, <a href=\"https://smythos.com/ai-agents/multi-agent-systems/multi-agent-systems-and-communication/\">https://smythos.com/ai-agents/multi-agent-systems/multi-agent-systems-and-communication/</a></p></li>\n<li><p>Preparing for the AI Agent Revolution: Navigating the Legal and Compliance Challenges of Autonomous Decision-Makers - StoneTurn, accessed April 14, 2025, <a href=\"https://stoneturn.com/insight/preparing-for-the-ai-agent-revolution/\">https://stoneturn.com/insight/preparing-for-the-ai-agent-revolution/</a></p></li>\n<li><p>Privacy Concerns AI Agent | ClickUp™, accessed April 14, 2025, <a href=\"https://clickup.com/p/ai-agents/privacy-concerns\">https://clickup.com/p/ai-agents/privacy-concerns</a></p></li>\n<li><p>Minding Mindful Machines: AI Agents and Data Protection Considerations, accessed April 14, 2025, <a href=\"https://fpf.org/blog/minding-mindful-machines-ai-agents-and-data-protection-considerations/\">https://fpf.org/blog/minding-mindful-machines-ai-agents-and-data-protection-considerations/</a></p></li>\n<li><p>Five privacy concerns around agentic AI | SC Media, accessed April 14, 2025, <a href=\"https://www.scworld.com/perspective/five-privacy-concerns-around-agentic-ai\">https://www.scworld.com/perspective/five-privacy-concerns-around-agentic-ai</a></p></li>\n<li><p>Unlocking the Potential of Agentic AI with Privacy-Enhancing Technologies - Duality Tech, accessed April 14, 2025, <a href=\"https://dualitytech.com/blog/unlocking-the-potential-of-agentic-ai-with-privacy-enhancing-technologies/\">https://dualitytech.com/blog/unlocking-the-potential-of-agentic-ai-with-privacy-enhancing-technologies/</a></p></li>\n<li><p>Empowering Agentic AI Within Financial Systems Requires Zero-Knowledge Proofs and Privacy-Preserving Technologies | Chainlink Blog, accessed April 14, 2025, <a href=\"https://blog.chain.link/agentic-ai-in-finance/\">https://blog.chain.link/agentic-ai-in-finance/</a></p></li>\n<li><p>Secret Network and Project Zero Partner, accessed April 14, 2025, <a href=\"https://scrt.network/blog/secret-network-and-project-zero-partner\">https://scrt.network/blog/secret-network-and-project-zero-partner</a></p></li>\n<li><p>AI Agents Need A Privacy Layer - Oasis Network, accessed April 14, 2025, <a href=\"https://oasisprotocol.org/blog/ai-agents-privacy-blockchain\">https://oasisprotocol.org/blog/ai-agents-privacy-blockchain</a></p></li>\n<li><p>The Rise of AI Agents and the Security Challenges Ahead | Auth0, accessed April 14, 2025, <a href=\"https://auth0.com/blog/the-rise-of-ai-agents-and-the-security-challenges-ahead/\">https://auth0.com/blog/the-rise-of-ai-agents-and-the-security-challenges-ahead/</a></p></li>\n<li><p>The Identities Behind AI Agents: A Deep Dive Into AI &amp; NHI - The Hacker News, accessed April 14, 2025, <a href=\"https://thehackernews.com/2025/04/the-identities-behind-ai-agents-deep.html\">https://thehackernews.com/2025/04/the-identities-behind-ai-agents-deep.html</a></p></li>\n<li><p>Mitigating the Top 10 Vulnerabilities in AI Agents - XenonStack, accessed April 14, 2025, <a href=\"https://www.xenonstack.com/blog/vulnerabilities-in-ai-agents\">https://www.xenonstack.com/blog/vulnerabilities-in-ai-agents</a></p></li>\n<li><p>Challenges in Governing AI Agents - Lawfare, accessed April 14, 2025, <a href=\"https://www.lawfaremedia.org/article/challenges-in-governing-ai-agents\">https://www.lawfaremedia.org/article/challenges-in-governing-ai-agents</a></p></li>\n<li><p>Awesome-LLM-based-AI-Agents-Knowledge/8-4-communication.md at main - GitHub, accessed April 14, 2025, <a href=\"https://github.com/mind-network/Awesome-LLM-based-AI-Agents-Knowledge/blob/main/8-4-communication.md\">https://github.com/mind-network/Awesome-LLM-based-AI-Agents-Knowledge/blob/main/8-4-communication.md</a></p></li>\n<li><p>5 Security Considerations for Managing AI Agents and Their Identities - Aembit, accessed April 14, 2025, <a href=\"https://aembit.io/blog/5-security-considerations-for-managing-ai-agents-and-their-identities/\">https://aembit.io/blog/5-security-considerations-for-managing-ai-agents-and-their-identities/</a></p></li>\n<li><p>Understanding AI Agent Security - Promptfoo, accessed April 14, 2025, <a href=\"https://www.promptfoo.dev/blog/agent-security/\">https://www.promptfoo.dev/blog/agent-security/</a></p></li>\n<li><p>The Future of Autonomous Agents: Trends, Challenges, and Opportunities Ahead, accessed April 14, 2025, <a href=\"https://smythos.com/ai-agents/agent-architectures/future-of-autonomous-agents/\">https://smythos.com/ai-agents/agent-architectures/future-of-autonomous-agents/</a></p></li>\n<li><p>AI Agent Best Practices and Ethical Considerations | Writesonic, accessed April 14, 2025, <a href=\"https://writesonic.com/blog/ai-agents-best-practices\">https://writesonic.com/blog/ai-agents-best-practices</a></p></li>\n<li><p>The Ethical Challenges of AI Agents | Tepperspectives, accessed April 14, 2025, <a href=\"https://tepperspectives.cmu.edu/all-articles/the-ethical-challenges-of-ai-agents/\">https://tepperspectives.cmu.edu/all-articles/the-ethical-challenges-of-ai-agents/</a></p></li>\n<li><p>Ethical considerations in deploying autonomous AI agents - Tech Edition, accessed April 14, 2025, <a href=\"https://www.techedt.com/ethical-considerations-in-deploying-autonomous-ai-agents\">https://www.techedt.com/ethical-considerations-in-deploying-autonomous-ai-agents</a></p></li>\n<li><p>Intent recognition in multi-agent systems: Cow herding - ResearchGate, accessed April 14, 2025, <a href=\"https://www.researchgate.net/publication/274205903_Intent_recognition_in_multi-agent_systems_Cow_herding\">https://www.researchgate.net/publication/274205903_Intent_recognition_in_multi-agent_systems_Cow_herding</a></p></li>\n<li><p>Intent Recognition in Multi-Agent Systems: Collective Box Pushing and Cow Herding - CORE, accessed April 14, 2025, <a href=\"https://core.ac.uk/download/pdf/213404087.pdf\">https://core.ac.uk/download/pdf/213404087.pdf</a></p></li>\n<li><p>Plan and Intent Recognition in a Multi-agent System for Collective Box Pushing, accessed April 14, 2025, <a href=\"https://www.researchgate.net/publication/274469749_Plan_and_Intent_Recognition_in_a_Multi-agent_System_for_Collective_Box_Pushing\">https://www.researchgate.net/publication/274469749_Plan_and_Intent_Recognition_in_a_Multi-agent_System_for_Collective_Box_Pushing</a></p></li>\n<li><p>Prediction of Intent in Robotics and Multi-agent Systems | SciSpace, accessed April 14, 2025, <a href=\"https://scispace.com/pdf/prediction-of-intent-in-robotics-and-multi-agent-systems-2uazbh7zl0.pdf\">https://scispace.com/pdf/prediction-of-intent-in-robotics-and-multi-agent-systems-2uazbh7zl0.pdf</a></p></li>\n<li><p>Negotiation Protocols for AI Agents - Matoffo, accessed April 14, 2025, <a href=\"https://matoffo.com/negotiation-protocols-for-ai-agents/\">https://matoffo.com/negotiation-protocols-for-ai-agents/</a></p></li>\n<li><p>accessed January 1, 1970, <a href=\"https://www.researchgate.net/publication/343780350_Autonomous_Negotiation_in_Multi-Agent_Systems_Principles_and_Challenges\">https://www.researchgate.net/publication/343780350_Autonomous_Negotiation_in_Multi-Agent_Systems_Principles_and_Challenges</a></p></li>\n<li><p>Agent Communication and Negotiation: Enhancing Decision-Making and Collaboration in Multi-Agent Systems - SmythOS, accessed April 14, 2025, <a href=\"https://smythos.com/ai-agents/agent-architectures/agent-communication-and-negotiation/\">https://smythos.com/ai-agents/agent-architectures/agent-communication-and-negotiation/</a></p></li>\n<li><p>Multi-Agent Systems and Negotiation: Strategies for Effective Agent Collaboration, accessed April 14, 2025, <a href=\"https://smythos.com/ai-agents/multi-agent-systems/multi-agent-systems-and-negotiation/\">https://smythos.com/ai-agents/multi-agent-systems/multi-agent-systems-and-negotiation/</a></p></li>\n<li><p>Conflict Resolution AI Agent | ClickUp™, accessed April 14, 2025, <a href=\"https://clickup.com/p/ai-agents/conflict-resolution\">https://clickup.com/p/ai-agents/conflict-resolution</a></p></li>\n<li><p>Normative conflict resolution through human–autonomous agent interaction - University of York, accessed April 14, 2025, <a href=\"https://pure.york.ac.uk/portal/files/116793996/1-s2.0-S2666659625000101-main.pdf\">https://pure.york.ac.uk/portal/files/116793996/1-s2.0-S2666659625000101-main.pdf</a></p></li>\n<li><p>Dealing With Ethical Conflicts In Autonomous Agents And Multi-Agent Systems, accessed April 14, 2025, <a href=\"https://www.researchgate.net/publication/279258407_Dealing_With_Ethical_Conflicts_In_Autonomous_Agents_And_Multi-Agent_Systems\">https://www.researchgate.net/publication/279258407_Dealing_With_Ethical_Conflicts_In_Autonomous_Agents_And_Multi-Agent_Systems</a></p></li>\n<li><p>Resolving Conflict in Decision-Making for Autonomous Driving - Robotics, accessed April 14, 2025, <a href=\"https://www.roboticsproceedings.org/rss17/p049.pdf\">https://www.roboticsproceedings.org/rss17/p049.pdf</a></p></li>\n<li><p>How can multi-agent systems communicate? Is game theory the answer? - Capgemini USA, accessed April 14, 2025, <a href=\"https://www.capgemini.com/us-en/insights/expert-perspectives/how-can-multi-agent-systems-communicate-is-game-theory-the-answer/\">https://www.capgemini.com/us-en/insights/expert-perspectives/how-can-multi-agent-systems-communicate-is-game-theory-the-answer/</a></p></li>\n<li><p>Agent-Based Modeling and Game Theory: Simulating Strategic Interactions in Complex Systems - SmythOS, accessed April 14, 2025, <a href=\"https://smythos.com/ai-industry-solutions/law/agent-based-modeling-and-game-theory/\">https://smythos.com/ai-industry-solutions/law/agent-based-modeling-and-game-theory/</a></p></li>\n<li><p>Game-theoretic LLM: Agent Workflow for Negotiation Games - arXiv, accessed April 14, 2025, <a href=\"https://arxiv.org/html/2411.05990v1\">https://arxiv.org/html/2411.05990v1</a></p></li>\n<li><p>Scientific approaches and techniques for negotiation : a game theoretic and artificial intelligence perspective - CWI, accessed April 14, 2025, <a href=\"https://ir.cwi.nl/pub/4448\">https://ir.cwi.nl/pub/4448</a></p></li>\n<li><p>Game Theory (Stanford Encyclopedia of Philosophy), accessed April 14, 2025, <a href=\"https://plato.stanford.edu/entries/game-theory/#AI\">https://plato.stanford.edu/entries/game-theory/#AI</a></p></li>\n<li><p>Game Theory in AI: The Nash Equilibrium EXPLAINED - YouTube, accessed April 14, 2025, <a href=\"https://www.youtube.com/watch?v=fbHl9AbcSic\">https://www.youtube.com/watch?v=fbHl9AbcSic</a></p></li>\n<li><p>MARLIN: Multi-Agent Reinforcement Learning Guided by Language-Based Inter-Robot Negotiation - arXiv, accessed April 14, 2025, <a href=\"https://arxiv.org/html/2410.14383v3\">https://arxiv.org/html/2410.14383v3</a></p></li>\n<li><p>[2410.14383] MARLIN: Multi-Agent Reinforcement Learning Guided by Language-Based Inter-Robot Negotiation - arXiv, accessed April 14, 2025, <a href=\"https://arxiv.org/abs/2410.14383\">https://arxiv.org/abs/2410.14383</a></p></li>\n<li><p>Applying Multi-Agent Reinforcement Learning to Candidate/Employer Job Matching and Salary Negotiations | Computer Science and Economics, accessed April 14, 2025, <a href=\"https://csec.yale.edu/senior-essays/fall-2022/applying-multi-agent-reinforcement-learning-candidateemployer-job-matching\">https://csec.yale.edu/senior-essays/fall-2022/applying-multi-agent-reinforcement-learning-candidateemployer-job-matching</a></p></li>\n<li><p>Deep Reinforcement Learning Agent for Negotiation in Multi-Agent Cooperative Distributed Predictive Control - MDPI, accessed April 14, 2025, <a href=\"https://www.mdpi.com/2076-3417/13/4/2432\">https://www.mdpi.com/2076-3417/13/4/2432</a></p></li>\n<li><p>MULTI-AGENT REINFORCEMENT LEARNING FOR COALITIONAL BARGAINING GAMES, accessed April 14, 2025, <a href=\"https://openreview.net/forum?id=OaZktJBVpUy\">https://openreview.net/forum?id=OaZktJBVpUy</a></p></li>\n<li><p>A Deep Reinforcement Learning Approach to Concurrent Bilateral Negotiation - IJCAI, accessed April 14, 2025, <a href=\"https://www.ijcai.org/proceedings/2020/0042.pdf\">https://www.ijcai.org/proceedings/2020/0042.pdf</a></p></li>\n<li><p>Towards Learning Multi-Agent Negotiations via Self-Play - CVF Open Access, accessed April 14, 2025, <a href=\"https://openaccess.thecvf.com/content_ICCVW_2019/papers/ADW/Tang_Towards_Learning_Multi-Agent_Negotiations_via_Self-Play_ICCVW_2019_paper.pdf\">https://openaccess.thecvf.com/content_ICCVW_2019/papers/ADW/Tang_Towards_Learning_Multi-Agent_Negotiations_via_Self-Play_ICCVW_2019_paper.pdf</a></p></li>\n<li><p>Towards Learning Multi-Agent Negotiations via Self-Play, accessed April 14, 2025, <a href=\"https://machinelearning.apple.com/research/towards-learning-multi-agent-negotiations-via-self-play\">https://machinelearning.apple.com/research/towards-learning-multi-agent-negotiations-via-self-play</a></p></li>\n<li><p>Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies - ACL Anthology, accessed April 14, 2025, <a href=\"https://aclanthology.org/P14-1047/\">https://aclanthology.org/P14-1047/</a></p></li>\n<li><p>accessed January 1, 1970, <a href=\"https://arxiv.org/abs/2006.03753\">https://arxiv.org/abs/2006.03753</a></p></li>\n<li><p>Unlock Savings with Autonomous Negotiation Agents (ANA) - Zycus, accessed April 14, 2025, <a href=\"https://www.zycus.com/solution/autonomous-negotiation-agents\">https://www.zycus.com/solution/autonomous-negotiation-agents</a></p></li>\n<li><p>Negotiation Strategy AI Agent | ClickUp™, accessed April 14, 2025, <a href=\"https://clickup.com/p/ai-agents/negotiation-strategy\">https://clickup.com/p/ai-agents/negotiation-strategy</a></p></li>\n<li><p>AI Negotiation Agent | statworx®, accessed April 14, 2025, <a href=\"https://www.statworx.com/en/generative-ai-solutions/ai-negotiation-agent/\">https://www.statworx.com/en/generative-ai-solutions/ai-negotiation-agent/</a></p></li>\n<li><p>AI-Powered Deals: How Autonomous Negotiation is Redefining Supply Chain Strategy, accessed April 14, 2025, <a href=\"https://supplychain360.io/autonomous-negotiation-revolutionizing-supply-chain-efficiency-2025-trends/\">https://supplychain360.io/autonomous-negotiation-revolutionizing-supply-chain-efficiency-2025-trends/</a></p></li>\n<li><p>Multi-AI Agents and How Business Can Prepare, accessed April 14, 2025, <a href=\"https://www.mri.co.jp/en/knowledge/article/202412_2.html\">https://www.mri.co.jp/en/knowledge/article/202412_2.html</a></p></li>\n<li><p>The Role of Procurement: AI Agents for Contract Negotiation in Finance - Akira AI, accessed April 14, 2025, <a href=\"https://www.akira.ai/blog/ai-agents-for-contract-negotiation\">https://www.akira.ai/blog/ai-agents-for-contract-negotiation</a></p></li>\n<li><p>AI Lease Negotiation 2025 Ultimate Guide | Real Estate Deals - Rapid Innovation, accessed April 14, 2025, <a href=\"https://www.rapidinnovation.io/post/ai-agent-lease-negotiation-assistant\">https://www.rapidinnovation.io/post/ai-agent-lease-negotiation-assistant</a></p></li>\n<li><p>AI Agents Are Transforming Healthcare Payer Interactions With Smart Negotiation, accessed April 14, 2025, <a href=\"https://www.thoughtful.ai/blog/ai-agents-are-transforming-healthcare-payer-interactions-with-smart-negotiation\">https://www.thoughtful.ai/blog/ai-agents-are-transforming-healthcare-payer-interactions-with-smart-negotiation</a></p></li>\n<li><p>Understanding Agentic AI in Procurement: How Autonomous AI Has Been Transforming Supplier Deals - Pactum, accessed April 14, 2025, <a href=\"https://pactum.com/understanding-agentic-ai-in-procurement-how-autonomous-ai-has-been-transforming-supplier-deals/\">https://pactum.com/understanding-agentic-ai-in-procurement-how-autonomous-ai-has-been-transforming-supplier-deals/</a></p></li>\n<li><p>When Will Your AI Negotiate With My AI? - Nibble, accessed April 14, 2025, <a href=\"https://blog.nibbletechnology.com/will-ai-negotiate-with-ai\">https://blog.nibbletechnology.com/will-ai-negotiate-with-ai</a></p></li>\n<li><p>Contract Negotiation AI Agents for the Finance Industry - Glide, accessed April 14, 2025, <a href=\"https://www.glideapps.com/agents/finance/contract-negotiation-ai-agents\">https://www.glideapps.com/agents/finance/contract-negotiation-ai-agents</a></p></li>\n<li><p>How To Use AI Negotiation To Get More Of What You Want | Lindy, accessed April 14, 2025, <a href=\"https://www.lindy.ai/blog/ai-negotiation\">https://www.lindy.ai/blog/ai-negotiation</a></p></li>\n<li><p>AI in Contract Negotiations (procurement) : r/legaltech - Reddit, accessed April 14, 2025, <a href=\"https://www.reddit.com/r/legaltech/comments/1i3eqtg/ai_in_contract_negotiations_procurement/\">https://www.reddit.com/r/legaltech/comments/1i3eqtg/ai_in_contract_negotiations_procurement/</a></p></li>\n<li><p>The leader in agentic AI for procurement for over half a decade, accessed April 14, 2025, <a href=\"https://pactum.com/\">https://pactum.com/</a></p></li>\n<li><p>The crucial role of humans in AI oversight - Cornerstone OnDemand, accessed April 14, 2025, <a href=\"https://www.cornerstoneondemand.com/resources/article/the-crucial-role-of-humans-in-ai-oversight/\">https://www.cornerstoneondemand.com/resources/article/the-crucial-role-of-humans-in-ai-oversight/</a></p></li>\n<li><p>How humans &amp; AI agents can work together ethically &amp; effectively - Macro 4, accessed April 14, 2025, <a href=\"https://www.macro4.com/blog/the-rise-of-ai-agents-how-humans-and-machines-can-work-together-ethically-and-effectively/\">https://www.macro4.com/blog/the-rise-of-ai-agents-how-humans-and-machines-can-work-together-ethically-and-effectively/</a></p></li>\n<li><p>New Ethics Risks Courtesy of AI Agents? Researchers Are on the Case - IBM, accessed April 14, 2025, <a href=\"https://www.ibm.com/think/insights/ai-agent-ethics\">https://www.ibm.com/think/insights/ai-agent-ethics</a></p></li>\n<li><p>What Ethical Issues Does Agentforce AI Bring to the Table for CIOs? - Inclusion Cloud, accessed April 14, 2025, <a href=\"https://inclusioncloud.com/insights/blog/ethical-issues-agentforce-cios/\">https://inclusioncloud.com/insights/blog/ethical-issues-agentforce-cios/</a></p></li>\n<li><p>AI agents evolve rapidly, challenging human oversight - IBM, accessed April 14, 2025, <a href=\"https://www.ibm.com/think/insights/ai-agents-evolve-rapidly\">https://www.ibm.com/think/insights/ai-agents-evolve-rapidly</a></p></li>\n<li><p>Unlocking value with AI agents: A responsible approach - PwC, accessed April 14, 2025, <a href=\"https://www.pwc.com/us/en/tech-effect/ai-analytics/responsible-ai-agents.html\">https://www.pwc.com/us/en/tech-effect/ai-analytics/responsible-ai-agents.html</a></p></li>\n<li><p>Human approval for AI agent actions - Safe Docs, accessed April 14, 2025, <a href=\"https://docs.safe.global/home/ai-agent-quickstarts/human-approval\">https://docs.safe.global/home/ai-agent-quickstarts/human-approval</a></p></li>\n<li><p>From Fine Print to Machine Code: How AI Agents are Rewriting the Rules of Engagement: Part 3 of 3, accessed April 14, 2025, <a href=\"https://law.stanford.edu/2025/03/26/from-fine-print-to-machine-code-how-ai-agents-are-rewriting-the-rules-of-engagement-part-3-of-3/\">https://law.stanford.edu/2025/03/26/from-fine-print-to-machine-code-how-ai-agents-are-rewriting-the-rules-of-engagement-part-3-of-3/</a></p></li>\n<li><p>5 Ways To Build a Trustworthy AI Agent - Salesforce, accessed April 14, 2025, <a href=\"https://www.salesforce.com/blog/trustworthy-ai-agent/\">https://www.salesforce.com/blog/trustworthy-ai-agent/</a></p></li>\n<li><p>How to use Azure AI Agent Service with function calling - Learn Microsoft, accessed April 14, 2025, <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/function-calling\">https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/function-calling</a></p></li>\n<li><p>Function-Calling vs Agents - Community.aws, accessed April 14, 2025, <a href=\"https://community.aws/content/2sryksE4Ga2hAsUksJZfnT8pJnr/function-calling-vs-agents\">https://community.aws/content/2sryksE4Ga2hAsUksJZfnT8pJnr/function-calling-vs-agents</a></p></li>\n<li><p>ReAct agents vs function calling agents - LeewayHertz, accessed April 14, 2025, <a href=\"https://www.leewayhertz.com/react-agents-vs-function-calling-agents/\">https://www.leewayhertz.com/react-agents-vs-function-calling-agents/</a></p></li>\n<li><p>Agent Communication and Message Passing: Streamlining Interaction and Data Exchange in Multi-Agent Systems - SmythOS, accessed April 14, 2025, <a href=\"https://smythos.com/ai-agents/agent-architectures/agent-communication-and-message-passing/\">https://smythos.com/ai-agents/agent-architectures/agent-communication-and-message-passing/</a></p></li>\n<li><p>AI-Exchange Protocol (AIXP): A Communication Standard for Artificial Intelligence Agents - GitHub, accessed April 14, 2025, <a href=\"https://github.com/davila7/AIXP\">https://github.com/davila7/AIXP</a></p></li>\n<li><p>Communicating with other agents – Fetch.ai Documentation, accessed April 14, 2025, <a href=\"https://fetch.ai/docs/guides/agents/intermediate/communicating-with-other-agents\">https://fetch.ai/docs/guides/agents/intermediate/communicating-with-other-agents</a></p></li>\n<li><p>Designing AI Agents That Work for You, Part 1: Communication Patterns - Innovation at Consumer Reports, accessed April 14, 2025, <a href=\"https://innovation.consumerreports.org/designing-ai-agents-that-work-for-you-part-1/\">https://innovation.consumerreports.org/designing-ai-agents-that-work-for-you-part-1/</a></p></li>\n<li><p>How to Automate Meeting Scheduling with AI - Datagrid, accessed April 14, 2025, <a href=\"https://www.datagrid.com/blog/automate-email-scheduling-ai\">https://www.datagrid.com/blog/automate-email-scheduling-ai</a></p></li>\n<li><p>Calendly AI Agents - Relevance AI, accessed April 14, 2025, <a href=\"https://relevanceai.com/agent-templates-software/calendly\">https://relevanceai.com/agent-templates-software/calendly</a></p></li>\n<li><p>Emergency Meeting Coordination AI Agent | ClickUp™, accessed April 14, 2025, <a href=\"https://clickup.com/p/ai-agents/emergency-meeting-coordination\">https://clickup.com/p/ai-agents/emergency-meeting-coordination</a></p></li>\n<li><p>www.salesforce.com, accessed April 14, 2025, <a href=\"https://www.salesforce.com/service/ai/customer-service-agents/#:~:text=AI%20customer%20service%20agents%20are,a%20personalized%20and%20conversational%20way.\">https://www.salesforce.com/service/ai/customer-service-agents/#:~:text=AI%20customer%20service%20agents%20are,a%20personalized%20and%20conversational%20way.</a></p></li>\n<li><p>AI Customer Service Agents - Salesforce, accessed April 14, 2025, <a href=\"https://www.salesforce.com/service/ai/customer-service-agents/\">https://www.salesforce.com/service/ai/customer-service-agents/</a></p></li>\n<li><p>AI Agent-Led Customer Service: Revolutionizing Support with Freddy AI - Freshworks, accessed April 14, 2025, <a href=\"https://www.freshworks.com/freshdesk/ai-agents/customer-service/\">https://www.freshworks.com/freshdesk/ai-agents/customer-service/</a></p></li>\n<li><p>sema4.ai, accessed April 14, 2025, <a href=\"https://sema4.ai/blog/ai-agents-supply-chain/#:~:text=AI%20agents%20monitor%20supplier%20performance,costs%20while%20ensuring%20adequate%20supply.\">https://sema4.ai/blog/ai-agents-supply-chain/#:~:text=AI%20agents%20monitor%20supplier%20performance,costs%20while%20ensuring%20adequate%20supply.</a></p></li>\n<li><p>AI Agents for Manufacturing Success | Salesforce US, accessed April 14, 2025, <a href=\"https://www.salesforce.com/manufacturing/artificial-intelligence/ai-agents-for-manufacturing/\">https://www.salesforce.com/manufacturing/artificial-intelligence/ai-agents-for-manufacturing/</a></p></li>\n<li><p>Revolutionizing Supply Chain Management: How AI Agents are Reshaping Industry Logistics - Sema4.ai, accessed April 14, 2025, <a href=\"https://sema4.ai/blog/ai-agents-supply-chain/\">https://sema4.ai/blog/ai-agents-supply-chain/</a></p></li>\n<li><p>Industrial AI in action: How AI agents and digital threads will transform the manufacturing industries - Microsoft, accessed April 14, 2025, <a href=\"https://www.microsoft.com/en-us/industry/blog/manufacturing-and-mobility/manufacturing/2025/03/25/industrial-ai-in-action-how-ai-agents-and-digital-threads-will-transform-the-manufacturing-industries/\">https://www.microsoft.com/en-us/industry/blog/manufacturing-and-mobility/manufacturing/2025/03/25/industrial-ai-in-action-how-ai-agents-and-digital-threads-will-transform-the-manufacturing-industries/</a></p></li>\n<li><p>From Data to Decisions: AI Agents for Industrial Process Optimization - Akira AI, accessed April 14, 2025, <a href=\"https://www.akira.ai/blog/ai-agents-for-industrial-process-optimization\">https://www.akira.ai/blog/ai-agents-for-industrial-process-optimization</a></p></li>\n<li><p>Why should manufacturers embrace AI agents now? - The World Economic Forum, accessed April 14, 2025, <a href=\"https://www.weforum.org/stories/2025/01/why-manufacturers-should-embrace-next-frontier-ai-agents/\">https://www.weforum.org/stories/2025/01/why-manufacturers-should-embrace-next-frontier-ai-agents/</a></p></li>\n<li><p>AI Agents in Manufacturing 2025 Ultimate Guide - Rapid Innovation, accessed April 14, 2025, <a href=\"https://www.rapidinnovation.io/post/ai-agent-manufacturing-applications-use-cases-benefits\">https://www.rapidinnovation.io/post/ai-agent-manufacturing-applications-use-cases-benefits</a></p></li>\n<li><p>AI Agents for Manufacturing Will Give You Superpowers | Plataine, accessed April 14, 2025, <a href=\"https://www.plataine.com/blog/ai-agents-for-manufacturing-will-give-you-superpowers/\">https://www.plataine.com/blog/ai-agents-for-manufacturing-will-give-you-superpowers/</a></p></li>\n<li><p>What are AI Agents in Manufacturing? - Augmentir, accessed April 14, 2025, <a href=\"https://www.augmentir.com/glossary/ai-agents-in-manufacturing\">https://www.augmentir.com/glossary/ai-agents-in-manufacturing</a></p></li>\n<li><p>AI agent for manufacturing: Applications and use cases, components, capabilities, implementation and benefits - LeewayHertz, accessed April 14, 2025, <a href=\"https://www.leewayhertz.com/ai-agent-for-manufacturing/\">https://www.leewayhertz.com/ai-agent-for-manufacturing/</a></p></li>\n<li><p>Reinventing Manufacturing with Agentic AI - Akira AI, accessed April 14, 2025, <a href=\"https://www.akira.ai/blog/ai-agents-for-manufacturing\">https://www.akira.ai/blog/ai-agents-for-manufacturing</a></p></li>\n<li><p>AI Agents In Production – A High Level Overview - Hiflylabs, accessed April 14, 2025, <a href=\"https://hiflylabs.com/blog/2024/8/1/ai-agents-multi-agent-overview\">https://hiflylabs.com/blog/2024/8/1/ai-agents-multi-agent-overview</a></p></li>\n<li><p>How AI Agents Are Driving ROI: 3 Real-World Case Studies (2025) - Creole Studios, accessed April 14, 2025, <a href=\"https://www.creolestudios.com/real-world-ai-agent-case-studies/\">https://www.creolestudios.com/real-world-ai-agent-case-studies/</a></p></li>\n<li><p>The Future of AI: The Power of Agent-to-Agent - Workday Blog, accessed April 14, 2025, <a href=\"https://blog.workday.com/en-us/agent-to-agent-overview.html\">https://blog.workday.com/en-us/agent-to-agent-overview.html</a></p></li>\n<li><p>Future of AI Agents: Trends &amp; Predictions for Businesses (2025) - REVE Chat, accessed April 14, 2025, <a href=\"https://www.revechat.com/blog/future-of-ai-agents/\">https://www.revechat.com/blog/future-of-ai-agents/</a></p></li>\n<li><p>AI Agents: The Defining Workforce Trend of 2025 - Data Society, accessed April 14, 2025, <a href=\"https://datasociety.com/ai-agents-the-defining-workforce-trend-of-2025/\">https://datasociety.com/ai-agents-the-defining-workforce-trend-of-2025/</a></p></li>\n<li><p>Generative AI meets the virtual world: A model for human-AI collaboration - Deloitte, accessed April 14, 2025, <a href=\"https://www2.deloitte.com/us/en/insights/industry/technology/ai-and-vr-model-for-human-ai-collaboration.html\">https://www2.deloitte.com/us/en/insights/industry/technology/ai-and-vr-model-for-human-ai-collaboration.html</a></p></li>\n<li><p>Top 10 AI Agent Trends and Predictions for 2025 - Analytics Vidhya, accessed April 14, 2025, <a href=\"https://www.analyticsvidhya.com/blog/2024/12/ai-agent-trends/\">https://www.analyticsvidhya.com/blog/2024/12/ai-agent-trends/</a></p></li>\n<li><p>Why agents are the next frontier of generative AI - McKinsey, accessed April 14, 2025, <a href=\"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/why-agents-are-the-next-frontier-of-generative-ai\">https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/why-agents-are-the-next-frontier-of-generative-ai</a></p></li>\n<li><p>16 Real-World AI Agents Examples in 2025 - Aisera, accessed April 14, 2025, <a href=\"https://aisera.com/blog/ai-agents-examples/\">https://aisera.com/blog/ai-agents-examples/</a></p></li>\n</ol>\n",
            "image": "https://roger.rogverse.fyi/media/posts/11/Flux_Schnell_two_robots_talking_over_the_phone_in_1940_cartoon_3.jpg",
            "author": {
                "name": "Roger Filomeno"
            },
            "tags": [
                   "Blog",
                   "AI"
            ],
            "date_published": "2025-04-14T09:13:26+08:00",
            "date_modified": "2025-04-14T09:43:58+08:00"
        },
        {
            "id": "https://roger.rogverse.fyi/joplin-with-plugins-the-last-note-taking-app-you-will-need.html",
            "url": "https://roger.rogverse.fyi/joplin-with-plugins-the-last-note-taking-app-you-will-need.html",
            "title": "Joplin with plugins: The Last Note-Taking App You Will Need ",
            "summary": "The digital landscape is saturated with note-taking applications, each promising to be the ultimate solution for organizing thoughts, managing projects, and boosting productivity. From the all-encompassing workspace of Notion to the established ecosystem of Evernote, the interconnected graph of Obsidian, and the simplicity of Google&hellip;",
            "content_html": "\n  <p>\n    The digital landscape is saturated with note-taking applications, each promising to be the ultimate solution for organizing thoughts, managing projects, and boosting productivity. From the all-encompassing workspace of Notion to the established ecosystem of Evernote, the interconnected graph of Obsidian, and the simplicity of Google Keep, users have a plethora of options. However, amidst these giants, a lesser-known open-source application, <a href=\"https://joplinapp.org/\">Joplin</a>, is quietly amassing power through a unique advantage: its extensive and versatile plugin ecosystem. While often positioned as an underdog, Joplin's ability to be customized and extended through these community-driven additions allows it to not only compete with but, in many ways, surpass the functionality offered by its more mainstream counterparts.&nbsp;&nbsp;\n  </p>\n<div><div class=\"post__iframe\"><iframe loading=\"lazy\" width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ARNt7LFpzpw?si=7NVmCzTwdTiB4zwO\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div></div>\n\n  <p>\n    The core strength of Joplin lies in its adaptability. Unlike some note-taking applications that offer a fixed set of features, Joplin's architecture allows users to selectively enhance its capabilities through plugins. This modular approach provides a significant advantage, enabling individuals to tailor the application precisely to their specific needs and workflows. Instead of being forced to adopt features they don't require, Joplin users can choose from a vast library of extensions that add functionalities ranging from advanced project management tools to specialized academic features. This power of extensibility is Joplin's hidden weapon in its quest to challenge the dominance of the established note-taking giants.\n  </p>\n\n    <h2 id=\"joplins-arsenal-exploring-the-plugin-ecosystem\">\n      Joplin's Arsenal: Exploring the Plugin Ecosystem\n    </h2>\n\n  <p>\n    The true potential of Joplin is unlocked by its thriving <a href=\"https://joplinapp.org/plugins\" target=\"_blank\">plugin ecosystem</a>, which currently boasts over 150 extensions. This impressive number is a clear indicator of the active and dedicated community that supports and enhances the application. Installing these plugins is a remarkably user-friendly process. Many plugins can be found and installed directly from within the Joplin application itself through the settings menu. For users who prefer a more hands-on approach or want to explore the latest community contributions, plugins can also be downloaded from platforms like GitHub and installed manually. This dual approach to installation caters to users of all technical skill levels.\n  </p>\n\n  <p>\n    The breadth of Joplin's plugin ecosystem is truly remarkable, covering a wide spectrum of functionalities designed to enhance productivity, organization, customization, task management, and integration with other tools. For productivity enhancements, plugins like Note Tabs address a common desire to work with multiple notes simultaneously by introducing a familiar tabbed interface. Rich Markdown elevates the writing experience by offering a more visually appealing and customizable Markdown editor. Navigating long documents becomes easier with the Outline plugin, which provides a sidebar table of contents. Consistent formatting is ensured by the Templates plugin, allowing users to create and reuse predefined note structures. Quick access to frequently used items is provided by the Favorites plugin , while the Quick Links plugin streamlines internal linking between notes, fostering a more interconnected knowledge base.\n  </p>\n<div><p><img loading=\"lazy\" src=\"https://cdn.rogverse.fyi/thorium_FwXeqlTMIX.png\"  data-is-external-image=\"true\"></p></div>\n\n  <p>\n    For users seeking advanced organization capabilities, Joplin offers powerful plugin options. Kanban plugins, such as KanMug and YesYouKan, transform Joplin into a visual task management system, allowing notes to be arranged on customizable boards, similar to Notion's board views or dedicated Kanban applications. To visualize the connections between notes, Graph View plugins like Sepremento's Awesome Graph, Link Graph UI, and Joplin Graph Plugin offer interactive network diagrams, akin to Obsidian's renowned graph view. Enhancing the interconnectedness of notes further, Backlinks plugins like Automatic Backlinks automatically create links from notes that reference the current one, a core feature of Roam Research's approach to knowledge management.\n  </p>\n\n  <p>\n    The visual appeal and user experience of Joplin can also be tailored through plugins. The macOS Theme provides a native look and feel for macOS users , while the Extra Markdown editor settings plugin offers fine-grained control over the editing environment. Beyond specific themes, Joplin's core functionality supports the use of custom themes, allowing for deep visual customization.\n  </p>\n\n  <p>\n    Task management within Joplin can be significantly enhanced with dedicated plugins. Inline TODO allows embedding to-dos directly within notes and provides a consolidated summary. Metis offers a straightforward task manager based on the portable Todo.txt format. For tracking time spent on tasks, the Time Slip plugin provides a solution integrated directly within notes. Calendar integration is offered by the Joplin Calendar plugin , while the Journal plugin facilitates daily journaling practices. The Agenda plugin provides a dedicated panel displaying upcoming and overdue tasks.\n  </p>\n\n  <p>\n    Finally, Joplin's plugin ecosystem extends its reach through integrations with other valuable tools. The Web Clipper browser extension allows users to save web pages and articles as notes directly into Joplin. The Email to Note plugin enables sending emails to a specific address, which are then automatically converted into Joplin notes. For users of the spaced repetition learning tool Anki, the Anki Sync plugin provides bidirectional synchronization. Developers can even integrate Joplin with their coding workflow using the VSCode Integration.\n  </p>\n\n  <p>\n    Beyond these core categories, Joplin's plugin repository contains unique and specialized tools that highlight its remarkable flexibility. The Rubi and Furigana Plugin was created to assist users learning Japanese by adding support for ruby characters. The Function Plot Plugin provides the ability to generate mathematical visualizations directly within notes. The Hotfolder plugin automates the process of importing files from a local directory as new notes. The Joplin AI assistant integrates artificial intelligence capabilities for features like related note suggestions. For users who need spreadsheet-like functionality, the JSheets plugin offers a unique solution. Visual thinkers can benefit from the Mindmap Plugin. Lastly, the Automatic Backlinks plugin provides a crucial feature for building interconnected knowledge graphs, a hallmark of applications like Roam Research and Obsidian. These examples demonstrate the platform's capacity to cater to highly specific and diverse user needs.\n  </p>\n\n    <h2 id=\"the-giants-and-their-domains-strengths-and-weaknesses-of-alternatives\">\n      The Giants and Their Domains: Strengths and Weaknesses of Alternatives\n    </h2>\n\n  <p>\n    To truly understand how Joplin, empowered by its plugins, can compete, it's essential to examine the strengths and weaknesses of its primary alternatives. Each application offers a distinct set of features and caters to different user preferences.\n  </p>\n<div><p>\n<table>\n        <thead>\n            <tr>\n                <th>Application</th>\n                <th>Key Strengths</th>\n                <th>Key Weaknesses</th>\n            </tr>\n        </thead>\n        <tbody>\n            <tr>\n                <td>Notion</td>\n                <td>Flexibility, databases, project management features, integrations, feature-rich free plan</td>\n                <td>Steep learning curve, potential for distraction, expensive for teams</td>\n            </tr>\n            <tr>\n                <td>Evernote</td>\n                <td>Web clipping, organization, search, integrations, robust note-taking features</td>\n                <td>Restricted free plan, high pricing, performance issues reported</td>\n            </tr>\n            <tr>\n                <td>Obsidian</td>\n                <td>Local-first, Markdown-based, extensive plugin ecosystem, graph view, strong internal linking</td>\n                <td>Steeper learning curve for non-technical users, less intuitive for basic note-taking, paid sync service</td>\n            </tr>\n            <tr>\n                <td>Microsoft OneNote</td>\n                <td>Free, cross-platform, integration with Microsoft ecosystem, inking capabilities, flexible page layout</td>\n                <td>Can feel bloated, syncing issues reported, less privacy-focused</td>\n            </tr>\n            <tr>\n                <td>Apple Notes</td>\n                <td>Free for Apple users, integrated into the ecosystem, simple and fast, secure notes</td>\n                <td>Limited features compared to others, only available on Apple devices, syncing issues reported</td>\n            </tr>\n            <tr>\n                <td>Google Keep</td>\n                <td>Free, simple, cross-platform, quick note-taking, reminders, excellent Google Workspace integration</td>\n                <td>Limited formatting and organization, no notebooks, basic task management, no dedicated desktop app</td>\n            </tr>\n            <tr>\n                <td>Roam Research</td>\n                <td>Bi-directional linking, networked thought, graph view, focus on emergent connections</td>\n                <td>Expensive, steep learning curve, cloud-based (privacy concerns for some), development concerns</td>\n            </tr>\n            <tr>\n                <td>Logseq</td>\n                <td>Free and open-source, local-first, outliner-based, bi-directional linking, flashcards, PDF annotation</td>\n                <td>Steeper learning curve, less polished than some, mobile apps in beta</td>\n            </tr>\n            <tr>\n                <td>Craft</td>\n                <td>Beautiful interface, block-based editor, collaboration features, well-structured documents</td>\n                <td>Primarily cloud-based, less focus on plain Markdown, can be expensive for individual users after the free tier</td>\n            </tr>\n        </tbody>\n    </table>\n</p></div>\n\n  <p>\n    Notion excels as a versatile workspace, offering a highly flexible block-based system that allows users to create custom pages, databases, and project management tools. Its strong collaboration features and integrations make it a popular choice for teams. However, its extensive functionality can be overwhelming for new users, leading to a steep learning curve. The sheer number of options can also be a source of distraction, and the pricing structure can become prohibitive for larger teams.&nbsp;\n  </p>\n\n  <p>\n    Evernote, a long-standing player in the note-taking space, offers robust note-taking features with rich formatting options and an excellent web clipper. Its powerful search functionality, including OCR, and good organizational structure with notebooks and tags are key strengths. However, the free plan has become increasingly restrictive, and the paid plans can be quite expensive. Some users have also reported performance issues and a feeling of the application becoming bloated over time.\n  </p>\n\n  <p>\n    Obsidian, favored by those building personal knowledge graphs, offers a local-first approach, ensuring data ownership and privacy. Built on Markdown, it provides flexibility and longevity for notes. Its highly customizable nature, thanks to a vast plugin ecosystem and themes, is a major draw. The powerful internal linking with backlinks and a graph view for visualizing connections are core features. However, users unfamiliar with Markdown and the concept of a personal knowledge graph might face a steeper learning curve. While the interface is minimalistic, it might not appeal to all users, and official syncing across devices requires a paid subscription.\n  </p>\n\n  <p>\n    Microsoft OneNote, often pre-installed on Windows devices, is free to use and integrates seamlessly with the Microsoft 365 ecosystem. Its flexible page layout allows for freeform note placement, and its strong inking capabilities are excellent for handwritten notes and annotations. However, some users find it can feel bloated with features, and syncing issues across devices are occasionally reported. Compared to some other options, it places less emphasis on privacy.\n  </p>\n\n  <p>\n    Apple Notes, free for Apple users, offers deep integration within the Apple ecosystem, providing seamless syncing across devices. Its user interface is simple and intuitive, making it excellent for quick note-taking, and it offers secure notes with Face ID/Touch ID locking. However, compared to more advanced applications, its feature set is limited, and it is only available within the Apple ecosystem. Syncing reliability has also been questioned by some users.\n  </p>\n\n  <p>\n    Google Keep stands out for its simplicity and speed in capturing quick notes, lists, and reminders, and it's completely free. Its excellent integration with other Google Workspace apps is a significant advantage for many users. However, it offers very limited formatting options and lacks a hierarchical organization system with notebooks. Its task management capabilities are basic, and it does not have a dedicated desktop application.\n  </p>\n\n  <p>\n    Roam Research has pioneered the concept of bi-directional linking, allowing users to create highly interconnected knowledge networks. Its graph view provides a unique way to visualize the relationships between notes, and it focuses on networked thought and discovering emergent connections. However, it is one of the most expensive note-taking applications available and can have a steep learning curve due to its unconventional approach. Its reliance on cloud storage might also be a concern for privacy-conscious users.\n  </p>\n\n  <p>\n    Logseq, a free and open-source alternative, offers a local-first approach, prioritizing data privacy. Its outliner-based structure encourages breaking down thoughts into interconnected blocks, and it features strong bi-directional linking and graph view capabilities. It also includes useful features like flashcards and PDF annotation. However, users unfamiliar with outliner workflows might experience a steeper learning curve, and its user interface might feel less polished than some commercial options. The mobile apps are currently in beta.\n  </p>\n\n  <p>\n    Craft provides a visually appealing and user-friendly interface with a block-based editor focused on creating well-structured documents. Its strong collaboration features allow for real-time sharing and co-editing of documents. However, it is primarily cloud-based, which might concern users who prefer local storage. It also places less emphasis on plain Markdown editing compared to some other applications and can become expensive for individual users after the free tier.\n  </p>\n\n    <h2 id=\"andnbspplugin-power-joplin-bridges-the-gaps-and-leaps-ahead\">\n      &nbsp;Plugin Power: Joplin Bridges the Gaps (and Leaps Ahead)\n    </h2>\n\n  <p>\n    Joplin's plugin ecosystem provides a powerful mechanism to address many of the weaknesses found in these alternative applications and even introduce features that surpass their built-in capabilities.\n  </p>\n\n  <p>\n    For project management, Joplin plugins like <a href=\"https://joplinapp.org/plugins/plugin/joplin-plugin-kanmug/\" target=\"_blank\">KanMug </a>and YesYouKan bring robust Kanban board functionality, allowing users to visually manage tasks and projects within their notes. This effectively bridges the gap with applications like Notion that offer similar board views. The flexibility of these Joplin plugins, with customizable columns and the ability to associate notes based on tags or notebook paths , can even offer a more tailored project management experience.\n  </p>\n\n  <p>\n    In the realm of task management, Joplin's plugin arsenal goes beyond the basic to-do lists offered by Evernote or OneNote. Inline TODO allows embedding tasks with context , Metis offers a portable Todo.txt-based system , and Time Slip provides integrated time tracking. The addition of calendar and agenda views through plugins like Joplin Calendar and Agenda  further enhances Joplin's ability to manage tasks effectively, potentially exceeding the built-in features of many competitors.\n  </p>\n<div><p><img loading=\"lazy\" src=\"https://cdn.rogverse.fyi/joplin-graph.png\"  data-is-external-image=\"true\"></p></div>\n\n  <p>\n    For knowledge organization, Joplin's graph view plugins (<a href=\"https://joplinapp.org/plugins/plugin/org.sepremento.joplin.graph/\" target=\"_blank\">Sepremento's Awesome Graph</a>, Link Graph UI, Joplin Graph Plugin) provide a visual exploration of interconnected notes, directly mirroring a key strength of Obsidian. The Automatic Backlinks plugin implements Roam Research's innovative bi-directional linking system, fostering a highly connected personal knowledge base. These plugins empower Joplin users to build sophisticated knowledge networks with visual exploration capabilities similar to dedicated tools.&nbsp;\n  </p>\n\n  <p>\n    While Joplin Cloud offers collaboration features, the plugin ecosystem is still evolving in this area. However, the ability to share notes via links and export in various formats provides workarounds for collaborative editing using other platforms.\n  </p>\n\n    <h2 id=\"andnbspvoices-from-the-community-user-reviews-and-comparisons\">\n      &nbsp;Voices from the Community: User Reviews and Comparisons\n    </h2>\n\n  <p>\n    User reviews and community discussions provide valuable real-world perspectives on Joplin's capabilities. Many users appreciate Joplin's open-source nature, giving them control over their data. The local storage option is a significant advantage for privacy-conscious individuals. The support for Markdown is also a key draw for many. The power of Joplin's plugins is frequently cited as a major strength, allowing users to tailor the application to their specific needs. Users report successfully implementing features like note tabs and graph views through plugins, effectively addressing feature gaps with other applications.\n  </p>\n\n  <p>\n    Some users note that Joplin's user interface can feel less polished compared to some alternatives. However, the availability of themes, such as the macOS theme, allows for visual customization. While the mobile app is sometimes mentioned as having limitations, it still provides essential functionality for on-the-go access. The initial learning curve of Markdown is also a point of feedback for some new users. Overall, the community sentiment suggests that while Joplin might have some areas for improvement, its plugin ecosystem is a powerful tool that allows users to overcome many of these limitations and create a highly customized and effective note-taking solution. The open-source nature and active community foster a sense of continuous improvement and shared ownership of the application.\n  </p>\n\n    <h2 id=\"the-freedom-of-choice-joplins-customization-edge\">\n      The Freedom of Choice: Joplin's Customization Edge\n    </h2>\n\n  <p>\n    A key differentiator for Joplin is its commitment to user choice and customization. Unlike more integrated applications like Apple Notes or Google Keep, which offer a fixed set of features, Joplin's plugin-based architecture provides unparalleled flexibility. Users are empowered to build a note-taking system that precisely matches their workflow, selecting only the functionalities they require. This modularity avoids the feature bloat often found in all-in-one solutions and allows for a highly personalized user experience.\n  </p>\n\n  <p>\n    Beyond plugins, Joplin offers further avenues for customization. Users can alter the application's visual appearance through custom themes. For those who desire even finer control over styling, the userchrome.css file can be modified to adjust the user interface at a granular level. Additionally, Joplin allows for the customization of keyboard shortcuts, enabling users to optimize their workflow for maximum efficiency. This deep level of customization, extending beyond just features to encompass appearance and behavior, provides Joplin users with a unique level of control over their note-taking environment\n  </p>\n\n    <h2 id=\"conclusion-joplin-the-plugin-powered-contender\">\n      Conclusion: Joplin - The Plugin-Powered Contender\n    </h2>\n\n  <p>\n    In a crowded marketplace of note-taking applications, Joplin stands out as a powerful and adaptable contender, primarily due to its robust and versatile plugin ecosystem. While it might be considered an underdog compared to giants like Notion, Evernote, and Obsidian, Joplin's ability to be tailored to individual needs through these community-driven extensions allows it to effectively compete and, in many instances, surpass the functionality offered by its more established rivals. Its open-source nature, coupled with its deep customization options, provides users with a level of control and flexibility that is often absent in proprietary alternatives. For individuals who value the freedom to shape their tools to their exact specifications, Joplin, with its plugin-powered arsenal, emerges as a compelling and highly capable note-taking solution ready to conquer the digital workspace.\n  </p>",
            "image": "https://roger.rogverse.fyi/media/posts/10/joplin-screenshot1.png",
            "author": {
                "name": "Roger Filomeno"
            },
            "tags": [
                   "note-taking",
                   "Blog"
            ],
            "date_published": "2025-04-08T23:02:27+08:00",
            "date_modified": "2025-04-08T23:42:13+08:00"
        },
        {
            "id": "https://roger.rogverse.fyi/neurosync-glimpse-interactive-digital-experiences.html",
            "url": "https://roger.rogverse.fyi/neurosync-glimpse-interactive-digital-experiences.html",
            "title": "NeuroSync: Glimpse Interactive Digital Experiences",
            "summary": "NeuroSync: The Future of Interactive Digital Experiences is Here Get ready for a revolution in digital interaction! This blog post dives into the exciting world of NeuroSync, an open-source project poised to redefine how we experience interactive games, digital avatars, and even streaming content. Prepare&hellip;",
            "content_html": "<h1 id=\"neurosync-the-future-of-interactive-digital-experiences-is-here\">NeuroSync: The Future of Interactive Digital Experiences is Here</h1>\n<p>Get ready for a revolution in digital interaction! This blog post dives into the exciting world of <a href=\"https://neurosync.info/\">NeuroSync</a>, an open-source project poised to redefine how we experience interactive games, digital avatars, and even streaming content. Prepare to have your perception of reality in the digital realm challenged!</p><h2 id=\"the-quest-for-believable-digital-avatars\">The Quest for Believable Digital Avatars</h2>\n<p>We’re constantly seeking more immersive and dynamic digital experiences. Whether it’s diving into a new game, exploring the metaverse, or connecting on social media, the believability of digital avatars is key. Realistic facial animation, with all its subtle nuances, is crucial for conveying emotions and creating genuine engagement. Historically, this has been a complex and labor-intensive process. But now, <a href=\"https://huggingface.co/AnimaVR/NEUROSYNC_Audio_To_Face_Blendshape\">NeuroSync</a> is stepping onto the scene to change the game.</p><p><figure class=\"post__image\"><img loading=\"lazy\" src=\"https://cdn.rogverse.fyi/inzoi.jpg\" alt=\"Digital Avatar\"  data-is-external-image=\"true\"></figure>\n<em>INZOI a glimpse into the world of digital avatars.</em></p><h2 id=\"neurosync-unlocking-real-time-facial-animation-in-unreal-engine-5\">NeuroSync: Unlocking Real-Time Facial Animation in Unreal Engine 5</h2>\n<p>NeuroSync is an open-source marvel that allows for the real-time streaming of facial blendshapes into the powerful <a href=\"https://www.unrealengine.com/en-US/\">Unreal Engine 5</a> using audio input. Let’s break down how this works:</p><h3 id=\"detailed-technical-overview\">Detailed Technical Overview</h3>\n<p>At its core, NeuroSync utilizes a sophisticated transformer seq2seq model. This model cleverly translates audio features into facial blendshape coefficients in real-time, making digital characters’ faces move in sync with their speech and even express emotions.</p><h3 id=\"the-power-of-the-local-api\">The Power of the Local API</h3>\n<p>For developers who crave control and minimal lag, NeuroSync offers a <a href=\"https://github.com/AnimaVR/NeuroSync_Local_API\">Local API</a>. This allows you to host the pre-trained audio-to-face model on your own hardware, giving you complete command over the animation process and potentially reducing latency.</p><h3 id=\"seamless-integration-with-unreal-engine-5\">Seamless Integration with Unreal Engine 5</h3>\n<p>Integrating NeuroSync into Unreal Engine 5 is a breeze thanks to the <a href=\"https://docs.unrealengine.com/5.3/en-US/livelink-in-unreal-engine/\">LiveLink API</a>. The <a href=\"https://github.com/AnimaVR/NeuroSync_Player\">NeuroSync Player</a> acts as the bridge, streaming the animation data directly into the engine. It leverages Apple’s ARKit blendshapes for a wide range of realistic facial movements.</p><h3 id=\"continuous-improvement\">Continuous Improvement</h3>\n<p><a href=\"https://youtu.be/iqzbrE7KV_M?si=pw4LWBluneJRI57i\">NeuroSync</a> is constantly evolving. Recent updates, like the one on February 4, 2025, have brought significant improvements in timing accuracy and the naturalness of expressions, especially in areas like brows, cheeks, and mouth shapes. Another update on March 29, 2025, further enhanced accuracy and smoothness through refined training data and model architecture.</p><p><figure class=\"post__image\"><img loading=\"lazy\" src=\"https://cdn.rogverse.fyi/thorium_m0BM7Eu1AP.png\" alt=\"NeuroSync in Unreal Engine 5\"  data-is-external-image=\"true\"></figure>\n<em>NeuroSync seamlessly integrates with Unreal Engine 5</em></p><h2 id=\"the-power-couple-neurosync-and-multimodal-llms-in-interactive-games\">The Power Couple: NeuroSync and Multimodal LLMs in Interactive Games</h2>\n<p>Imagine combining NeuroSync’s realistic facial animation with the intelligence of multimodal Large Language Models (LLMs). This synergy could lead to truly revolutionary interactive gaming experiences.</p><h3 id=\"beyond-text-multimodal-llms-understand-the-game\">Beyond Text: Multimodal LLMs Understand the Game</h3>\n<p>Multimodal LLMs can process various forms of data, including text, images, and audio. This allows them to understand the game’s context and the player’s input in a much richer way than traditional text-based LLMs. They can interpret visual cues, spoken dialogue, and even the overall game environment to create more intelligent and responsive NPCs.</p><h3 id=\"bringing-characters-to-life-with-expressive-animation\">Bringing Characters to Life with Expressive Animation</h3>\n<p>NeuroSync provides the visual expressiveness that complements the intelligence of MLLMs. While an MLLM can generate smart dialogue, NeuroSync animates the character’s face in real-time, synchronized with the speech. This creates incredibly believable and relatable virtual characters.</p><h3 id=\"the-future-is-now-existing-integrations\">The Future is Now: Existing Integrations</h3>\n<p>The integration of LLMs with game engines is already being explored. Projects like <a href=\"https://arxiv.org/html/2309.12276v2\">LLMR (Large Language Model for Mixed Reality)</a> and <a href=\"https://www.immersivecomputinglab.org/wp-content/uploads/2024/08/2024-ISMAR-AgentBehaviorGeneration.pdf\">VIVRA (Voice Interactive Virtual Reality Annotation)</a> showcase the potential of LLMs in creating dynamic and interactive virtual worlds. The <a href=\"https://github.com/AkshitIreddy/Interactive-LLM-Powered-NPCs\">Interactive LLM Powered NPCs</a> project even aims to revolutionize NPC interactions in existing games using AI for dialogue and facial animation.</p><p><figure class=\"post__image\"><img loading=\"lazy\" src=\"https://cdn.rogverse.fyi/graphic-features-supported-convai-1.png\" alt=\"LLM Powered NPC\"  data-is-external-image=\"true\"></figure>\n<em>Intelligent NPCs powered by LLMs.</em></p><h2 id=\"neurosync-and-metas-vision-digital-twins-and-ai-influencers\">NeuroSync and Meta’s Vision: Digital Twins and AI Influencers</h2>\n<p>Tech giant Meta is heavily invested in the metaverse and the creation of realistic digital twins and engaging AI influencers. NeuroSync could play a vital role in bringing these digital entities to life.</p><h3 id=\"metas-ambitious-plans\">Meta’s Ambitious Plans</h3>\n<p>Meta envisions a future where digital twins accurately represent real-world individuals in the metaverse. Engaging AI influencers would also require highly realistic and expressive avatars.</p><h3 id=\"enhancing-visual-fidelity-and-emotional-expression\">Enhancing Visual Fidelity and Emotional Expression</h3>\n<p>NeuroSync’s real-time audio-to-facial animation can significantly contribute to the visual fidelity and emotional expressiveness of these Meta-driven avatars. The ability to generate nuanced facial expressions directly from audio will make digital twins and AI influencers feel more alive and believable.</p><h3 id=\"potential-for-collaboration\">Potential for Collaboration</h3>\n<p>The synergy between NeuroSync and Meta’s goals opens up exciting possibilities for collaboration. Meta could integrate NeuroSync’s technology to enhance its avatar creation process, and the open-source nature of NeuroSync could benefit from the scale and resources of Meta’s platforms.</p><p><figure class=\"post__image\"><img loading=\"lazy\" src=\"https://cdn.rogverse.fyi/Zucc_pgzaxw.jpg\" alt=\"Metaverse Avatars\"  data-is-external-image=\"true\"></figure>\n<em>Realistic avatars in the metaverse.</em></p><h2 id=\"blurring-reality-neurosync-unreal-engine-5-and-streaming-content\">Blurring Reality: NeuroSync, Unreal Engine 5, and Streaming Content</h2>\n<p>The combination of NeuroSync and Unreal Engine 5’s incredible rendering capabilities could lead to a future where it’s hard to distinguish between computer-generated graphics and reality in streaming content.</p><h3 id=\"unreal-engine-5-a-master-of-photorealism\">Unreal Engine 5: A Master of Photorealism</h3>\n<p>Unreal Engine 5 is renowned for its ability to create stunningly photorealistic visuals in real-time. Games like Unrecord and Bodycam (while not explicitly detailed in the provided snippets, their visual fidelity is well-known) showcase the engine’s power to produce graphics that can often be mistaken for real life.</p><h3 id=\"elevating-ai-avatars-in-live-streaming\">Elevating AI Avatars in Live Streaming</h3>\n<p>When you pair UE5’s visual prowess with NeuroSync’s lifelike facial animation, AI avatars in live streaming scenarios can reach unprecedented levels of realism. Imagine AI streamers that not only look incredibly real but also speak and emote with natural facial expressions. This could truly blur the lines between virtual and human content creators.</p><p><figure class=\"post__image\"><img loading=\"lazy\" src=\"https://cdn.rogverse.fyi/Unreal+Engine_spotlights_meet-vincent-a-real-time-digital-human-created-in-house-by-a-team-of-just-five_Spotlight_Giantstep_blog_body_img2-1640x1000-fcfc6112efbd1cceabc7687e7fb4d456276791f8.jpg\" alt=\"Unreal Engine 5 Realism\"  data-is-external-image=\"true\"></figure>\n<em>Meet Vincent: a real-time digital human. The photorealistic power of Unreal Engine 5.</em></p><h2 id=\"the-rise-of-ai-digital-personalities-surpassing-humans\">The Rise of AI Digital Personalities: Surpassing Humans?</h2>\n<p>The emergence of AI digital personalities, such as the popular AI Vtubers Neurosama and Codemiko, raises the question: could AI eventually surpass human creators in online interactions?</p><h3 id=\"neurosama-and-codemiko-the-ai-vtubing-phenomenon\">Neurosama and Codemiko: The AI Vtubing Phenomenon</h3>\n<p><a href=\"https://en.wikipedia.org/wiki/Neuro-sama\">Neurosama</a> is an AI VTuber and chatbot on Twitch, created by developer Vedal. She uses a large language model to generate human-like responses and has gained a massive following, even breaking <a href=\"https://streamscharts.com/news/vedals-ai-vtuber-neuro-twitch-hype-train-record\">Twitch’s Hype Train record</a>. <a href=\"https://en.wikipedia.org/wiki/CodeMiko\">Codemiko</a>, on the other hand, is a VTuber known for her unique glitchy aesthetic and highly interactive streams. Behind the avatar is a real person, Youna Kang (The Technician), who uses motion capture and Unreal Engine to bring Codemiko to life.</p><h3 id=\"why-are-they-so-popular\">Why Are They So Popular?</h3>\n<p>Neurosama’s success is partly due to the novelty of interacting with an AI, while Codemiko’s appeal lies in her unique character and high level of interactivity. Both demonstrate that engaging digital personalities, whether fully AI-driven or human-controlled with advanced avatars, can captivate online audiences.</p><h3 id=\"ai-vs-humans-the-great-debate\">AI vs. Humans: The Great Debate</h3>\n<p>Could AI digital personalities truly surpass human creators? While AI offers advantages like 24/7 availability and scalability, it currently lacks the genuine emotions and lived experiences that drive deep human connection. It’s more likely that AI will become a powerful tool that complements human creativity rather than replacing it entirely.</p><p><figure class=\"post__image\"><img loading=\"lazy\" src=\"https://cdn.rogverse.fyi/Neuro-sama-20231106-1.webp\" alt=\"AI VTuber\"  data-is-external-image=\"true\"></figure>\n<em>AI VTubers like Neurosama are gaining popularity.</em></p><p><figure class=\"post__image\"><img loading=\"lazy\" src=\"https://cdn.rogverse.fyi/thorium_9XrxXIYuJO-ezgif.com-cut.gif\" alt=\"Codemiko\"  data-is-external-image=\"true\"></figure>\n<em>Codemiko, known for her interactive streams.</em></p><h2 id=\"conclusion-embracing-the-future-of-interactive-media\">Conclusion: Embracing the Future of Interactive Media</h2>\n<p>NeuroSync, in conjunction with the power of multimodal LLMs and the stunning visuals of Unreal Engine 5, is paving the way for a truly transformative era in interactive media. From more engaging games and realistic digital avatars to the potential blurring of lines in streaming content, the possibilities are immense. While AI digital personalities are making waves, the unique essence of human creativity will likely ensure a future where both AI and human creators thrive, offering diverse and enriching digital experiences.</p>",
            "image": "https://roger.rogverse.fyi/media/posts/9/neurosync-2.jpg",
            "author": {
                "name": "Roger Filomeno"
            },
            "tags": [
                   "Blog",
                   "AI"
            ],
            "date_published": "2025-04-07T11:20:23+08:00",
            "date_modified": "2025-04-07T15:05:23+08:00"
        },
        {
            "id": "https://roger.rogverse.fyi/beyond-the-terminal-is-vtm-the-revolutionary-text-based-desktop-in-the-making.html",
            "url": "https://roger.rogverse.fyi/beyond-the-terminal-is-vtm-the-revolutionary-text-based-desktop-in-the-making.html",
            "title": "Beyond the Terminal: Is VTM the Revolutionary Text-Based Desktop In The Making? ",
            "summary": "The world of command-line interfaces is experiencing a renaissance. For years, the terminal remained a relatively static tool, a direct descendant of the teletype machines of the past. But recently, we've seen exciting innovations aimed at enhancing the user experience, with tools like the Warp&hellip;",
            "content_html": "\n  <p>\n    The world of command-line interfaces is experiencing a renaissance. For years, the terminal remained a relatively static tool, a direct descendant of the teletype machines of the past. But recently, we've seen exciting innovations aimed at enhancing the user experience, with tools like the Warp terminal leading the charge . These modern terminals offer a plethora of features designed to boost productivity and make the command line more accessible. However, a new contender has emerged, one that proposes a far more radical shift: VTM, the text-based desktop environment . Could this be more than just another terminal emulator? Could it be a truly revolutionary way to interact with our computers?\n  </p>\n\n    <h2 id=\"warp-modernizing-the-terminal-experience\">\n      Warp: Modernizing the Terminal Experience\n    </h2>\n\n  <p>\n    Warp has garnered significant attention for its approach to modernizing the terminal. It takes the familiar concept of a terminal emulator and infuses it with features inspired by contemporary development environments . One of its standout capabilities is the integration of AI, offering intelligent command suggestions and explanations for errors, effectively lowering the barrier to entry for those less familiar with the command line . Warp also introduces the concept of \"blocks,\" grouping command inputs and outputs together for easier navigation, sharing, and filtering, addressing a common frustration of sifting through lengthy terminal sessions . For those accustomed to IDEs, Warp provides IDE-like editing on the command line, allowing users to place their mouse cursor and edit commands intuitively without the need for excessive backspacing . Furthermore, it boasts smart completions for a vast array of CLI tools, streamlining the process of entering commands . Collaboration is another key aspect, with features like session sharing enabling teams to work together on the command line in real-time . Users can also personalize their experience with extensive customization options for themes, keybindings, and prompts . Under the hood, Warp leverages technologies like Rust and GPU rendering to deliver performance enhancements, resulting in a faster and more responsive terminal . In essence, Warp refines the traditional terminal paradigm, making it more user-friendly, intelligent, and collaborative, focusing on improving the experience of interacting with the command line within a graphical environment .\n  </p>\n\n    <h2 id=\"vtm-a-different-breed-the-text-based-desktop-environment\">\n      VTM: A Different Breed - The Text-Based Desktop Environment\n    </h2>\n\n    <figure class=\"post__image post__image--center\">\n      <a href=\"https://github.com/directvt/vtm\" target=\"_blank\">\n        <img loading=\"lazy\" src=\"https://roger.rogverse.fyi/media/posts/7/vtm_jSrkwppU3f.png\" height=\"814\" width=\"1298\" alt=\"VTM on Windows 11 running Yazi\"  sizes=\"(min-width: 1500px) calc(7.87vw + 610px), (min-width: 900px) calc(44.48vw + 68px), (min-width: 780px) calc(8vw + 604px), calc(84.35vw + 23px)\" srcset=\"https://roger.rogverse.fyi/media/posts/7/responsive/vtm_jSrkwppU3f-xs.webp 300w ,https://roger.rogverse.fyi/media/posts/7/responsive/vtm_jSrkwppU3f-sm.webp 480w ,https://roger.rogverse.fyi/media/posts/7/responsive/vtm_jSrkwppU3f-md.webp 768w\">\n      </a>\n      <figcaption>https://github.com/directvt/vtm</figcaption>\n    </figure>\n\n  <p>\n    Stepping into a different realm entirely is VTM. Unlike Warp, which enhances the terminal, VTM aims to create a complete desktop environment constructed entirely from text cells arranged in a TUI (Text-based User Interface) matrix . This fundamental difference in approach leads to a unique set of features . The entire user interface in VTM is rendered using text characters, forming a mosaic of cells that constitute the display. This includes everything from application windows to any potential menus or interface elements. A key aspect of VTM is its ability to manage windows. It can wrap any console application within these text-based \"windows,\" which users can then arrange, resize, move, and even layer, offering a desktop-like organizational structure . The documentation for VTM's user interface details a comprehensive set of mouse and keyboard shortcuts for these window management operations, including actions for moving, resizing, maximizing, minimizing, and closing windows . Remarkably, VTM can be nested indefinitely, allowing for the creation of intricate and potentially isolated text-based desktop layouts . Users can launch existing command-line applications within this environment , with documentation mentioning command-line launching and the possibility of a default application setting . Video demonstrations show users launching terminal emulators and other text-based tools within the VTM environment . VTM offers flexibility in rendering its TUI matrix, capable of displaying it within its own GUI window (currently only on Windows) or within a compatible text console . Furthermore, VTM boasts cross-platform support, running on Windows and various \\*nix systems . This vision of a complete desktop built from text fundamentally diverges from the approach of traditional terminals and Warp, which primarily focus on enhancing the experience within the confines of a single terminal window. VTM, on the other hand, seeks to be a text-based alternative to the traditional graphical desktop.\n  </p>\n\n    <h2 id=\"revolutionary-potential-why-vtm-might-just-be-the-next-big-thing-or-a-niche-gem\">\n      Revolutionary Potential: Why VTM Might Just Be the Next Big Thing (or a Niche Gem)\n    </h2>\n\n  <p>\n    The implications of VTM being a text-based desktop environment are significant. One potential advantage lies in resource usage. Theoretically, an environment built entirely from text could consume considerably fewer system resources like CPU, RAM, and GPU compared to a conventional graphical desktop . While specific data on VTM's resource footprint isn't readily available, the inherent nature of a TUI suggests a lighter overhead compared to rendering complex graphical elements. This potential for lower resource consumption could make VTM particularly appealing in resource-constrained scenarios, on older hardware, or for users who prioritize efficiency.\n<br>\n<br>Furthermore, the text-based foundation of VTM could unlock unparalleled levels of customization. Since every aspect of the interface is rendered with text, users could theoretically have fine-grained control over its appearance and potentially even its functionality through text-based configuration files . The user interface documentation hints at extensive configuration options . This level of control could resonate with users who desire deep personalization of their computing environment.\n<br>\n<br>VTM's unique approach also opens up possibilities for use cases that extend beyond typical terminal workflows. It could serve as a minimalist computing environment for those seeking a distraction-free, text-centric experience. Its potential for lightweight operation could make it an efficient solution for remote access, especially in situations with limited bandwidth . VTM might also find applications in embedded systems where graphical capabilities are constrained. Additionally, it could offer unique accessibility advantages for users with certain visual impairments. The aesthetic of classic text-based interfaces combined with modern features like window management and application support could appeal to users who appreciate retro computing styles but require contemporary functionality . Moreover, VTM could provide a structured, windowed environment for running multiple terminal-based applications, offering a different organizational paradigm compared to tools like<code> tmux&nbsp;</code>or <code>screen</code> .\n<br>\n<br>In contrast, the enhancements offered by Warp primarily focus on improving the efficiency and usability of the traditional terminal experience within a graphical environment. While Warp undoubtedly addresses many pain points of command-line users, VTM challenges the fundamental paradigm of how we interact with our computers at a lower level. This difference in scope suggests that while Warp aims to make the existing terminal experience better, VTM explores a more radical alternative, potentially leading to more significant shifts in user interaction.\n  </p>\n\n    <h2 id=\"vtms-unique-features-standing-out-from-the-crowd\">\n      VTM's Unique Features: Standing Out from the Crowd\n    </h2>\n\n  <p>\n    Several specific features of VTM distinguish it from both traditional terminal emulators and modern ones like Warp. One key difference is VTM's text-based window management . Unlike the tabbed or split-pane interfaces common in traditional terminals, VTM provides a more flexible windowing system within its text grid. As demonstrated in videos, users can arrange applications in a manner reminiscent of a graphical desktop . The detailed user interface documentation further illustrates the extensive mouse and keyboard actions available for managing these windows . This approach to window management goes beyond the capabilities of terminal multiplexers like <code>tmux </code>or <code>screen</code>, offering a more visually oriented, albeit still text-based, method for organizing applications. Tools like <code>tmux </code>and <code>screen </code>typically operate within a single terminal window, whereas VTM creates distinct, resizable areas within its text-based desktop.\n<br>\n<br>Another unique feature is the ability to nest VTM instances . Running a text-based desktop environment within another offers a level of organizational control and potential for isolated workspaces not found in traditional terminals. This nesting capability could be particularly useful for managing complex workflows or creating sandboxed environments. Imagine having a dedicated text-based desktop for development tasks running inside another for general computing, providing a clear separation of concerns.\n<br>\n<br>VTM's capability to render its output to either a dedicated GUI window (currently on Windows) or a standard text console  provides significant adaptability. This dual rendering option makes VTM accessible in both graphical and purely text-based environments. For users who prefer a graphical interface, the Windows GUI rendering allows them to experience VTM's features, while those working in text-only environments can also utilize it. A video demonstrates VTM running seamlessly within the Windows Terminal , showcasing its ability to integrate with existing terminal emulators.\n<br>\n<br>Similar to <code>screen </code>or <code>tmux</code>, VTM allows users to detach from running applications, which continue to operate in the background, and then re-attach to them later . This feature enhances the persistence and management of long-running command-line tasks within the VTM environment. Users can close the VTM window, and their processes will continue to run, allowing them to resume their work at a later time.\n<br><br>Furthermore, VTM offers various mechanisms for remote access, enabling users to run remote applications and even the entire desktop environment over SSH . This built-in remote access functionality could make VTM a valuable tool for system administrators and developers who frequently work with remote machines. The documentation explicitly outlines commands for establishing remote connections and running applications or the full desktop via SSH.\n  </p>\n\n    <h2 id=\"why-you-might-want-to-take-vtm-for-a-spin\">\n      Why You Might Want to Take VTM for a Spin\n    </h2>\n\n  <p>\n    VTM presents several compelling reasons for users to consider trying it out, depending on their individual needs and preferences. For those who are simply curious about novel computing paradigms and enjoy exploring unconventional tools, VTM offers a unique experience unlike any standard terminal emulator or graphical desktop environment. Its text-based nature provides a distinct way of interacting with a computer that might appeal to those seeking something different.\n<br>\n<br>Users who value a minimalist, text-centric environment and are intrigued by the potential for lower resource usage compared to traditional GUIs might find VTM particularly attractive. If efficiency and a distraction-free workspace are priorities, VTM's approach could be a compelling alternative.\n<br>\n<br>Even for command-line power users who are already comfortable with tools like `tmux` or `screen`, VTM offers a different way to manage and interact with multiple terminal-based applications. The \"infinite canvas\" concept, as hinted at in video demonstrations , could provide a more intuitive or visually organized approach to managing numerous terminal sessions.\n<br>\n<br>Developers who spend a significant amount of time working with text editors, consoles, and other command-line tools might find VTM's integrated and organized workspace beneficial. Having the ability to arrange these tools in a desktop-like manner within a text-based environment could enhance productivity.\n<br>\n<br>Retro computing enthusiasts might appreciate VTM's ability to capture the aesthetic of classic text-based interfaces while providing modern features such as window management and cross-platform compatibility. It offers a nostalgic feel with contemporary functionality.\n<br>\n<br>System administrators could find VTM's remote access capabilities and its structured approach to managing multiple terminal sessions particularly useful for server management tasks. The ability to run remote desktops or individual applications over SSH could streamline remote workflows.\n<br>\n<br>However, it's important to acknowledge potential drawbacks. The entirely text-based nature of VTM might not appeal to users who heavily rely on graphical applications for their daily tasks. The user interface paradigm and the extensive reliance on keyboard shortcuts, as suggested in documentation and videos , might require a learning period for new users. The current limitation of GUI rendering to Windows might also be a consideration for users on other platforms who might need to run VTM within an existing terminal emulator. Finally, as VTM appears to be an active project, some features might be under development or subject to change.\n<br>\n<br>Ultimately, VTM caters to a specific niche of users who are either seeking a fundamentally different computing experience or have particular needs that a text-based desktop environment can address. While it's unlikely to replace mainstream graphical desktops for the majority of users, its unique approach could be highly valuable for certain individuals and specific use cases.\n  </p>\n\n    <h2 id=\"text-as-the-new-frontier\">\n      Text as the New Frontier?\n    </h2>\n\n  <p>\n    In the evolving landscape of command-line tools, Warp represents a significant step forward in enhancing the traditional terminal experience, making it more intelligent, user-friendly, and collaborative. However, VTM takes a far more radical approach by reimagining the entire desktop environment as a text-based interface. While Warp refines the familiar paradigm, VTM dares to ask \"what if we went back to text, but with modern windowing and application management?\". This bold approach could lead to exciting new ways of interacting with our computers, even if it remains a niche tool for specific use cases and adventurous users. For those intrigued by the possibilities of a text-based future, exploring the VTM GitHub repository and trying it out for themselves could offer a glimpse into a fundamentally different way of computing.\n  </p>\n\n    <h2 id=\"feature-comparison\">\n      Feature Comparison\n    </h2>\n<div><table>\n  <thead>\n    <tr>\n      <th>Feature</th>\n      <th>Warp</th>\n      <th>VTM (Text-based Desktop)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><strong>Core Concept</strong></td>\n      <td>Modern terminal emulator with enhanced features</td>\n      <td>Text-based application creating a complete desktop environment from text cells</td>\n    </tr>\n    <tr>\n      <td><strong>UI Paradigm</strong></td>\n      <td>Primarily graphical with text-based command line</td>\n      <td>Entirely text-based user interface (TUI)</td>\n    </tr>\n    <tr>\n      <td><strong>Window Management</strong></td>\n      <td>Tabs and split panes within a single terminal window</td>\n      <td>Text-based windows for applications that can be moved, resized, layered, and nested within a text grid</td>\n    </tr>\n    <tr>\n      <td><strong>Application Launching</strong></td>\n      <td>Standard command execution within the terminal</td>\n      <td>Wraps and runs any console application within its text-based windows</td>\n    </tr>\n    <tr>\n      <td><strong>AI Integration</strong></td>\n      <td>Built-in AI for command suggestions, error explanations, etc.</td>\n      <td>Not explicitly mentioned in the provided snippets.</td>\n    </tr>\n    <tr>\n      <td><strong>Collaboration</strong></td>\n      <td>Session sharing and other collaborative features</td>\n      <td>Multiple users can connect to a desktop session in real-time .</td>\n    </tr>\n    <tr>\n      <td><strong>Customization</strong></td>\n      <td>Themes, keybindings, prompts, etc.</td>\n      <td>Potentially extreme customization due to its text-based nature .</td>\n    </tr>\n    <tr>\n      <td><strong>Resource Usage</strong></td>\n      <td>Generally efficient, with GPU rendering for speed</td>\n      <td>Potentially lower resource usage compared to GUI environments (conceptual benefit of TUIs)</td>\n    </tr>\n    <tr>\n      <td><strong>Cross-Platform</strong></td>\n      <td>macOS, Linux, Windows</td>\n      <td>Windows, Linux, macOS, FreeBSD, NetBSD, OpenBSD</td>\n    </tr>\n    <tr>\n      <td><strong>GUI Rendering</strong></td>\n      <td>Yes</td>\n      <td>Dedicated GUI window available on Windows; renders within a terminal on other platforms</td>\n    </tr>\n    <tr>\n      <td><strong>Detached Processes</strong></td>\n      <td>Not a primary feature highlighted in snippets</td>\n      <td>Supports detaching and re-attaching to running applications .</td>\n    </tr>\n  </tbody>\n</table></div>",
            "image": "https://roger.rogverse.fyi/media/posts/7/Windows_Terminal_logo.svg-2.png",
            "author": {
                "name": "Roger Filomeno"
            },
            "tags": [
                   "Terminal",
                   "Blog"
            ],
            "date_published": "2025-03-23T19:03:03+08:00",
            "date_modified": "2025-03-23T19:21:45+08:00"
        },
        {
            "id": "https://roger.rogverse.fyi/crystalwire-10-release.html",
            "url": "https://roger.rogverse.fyi/crystalwire-10-release.html",
            "title": "Crystalwire 1.0 Release",
            "summary": "When you don't care about the firewall but need the jumping graphs that track which process consumes all your bandwidth, it's like Glasswire for terminals! Crystalwire is a command-line tool that monitors network bandwidth usage for each running process in real-time. It utilizes the psutil&hellip;",
            "content_html": "\n  <p>\n    When you don't care about the firewall but need the jumping graphs that track which process consumes all your bandwidth, it's like Glasswire for terminals!\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://roger.rogverse.fyi/media/posts/4/image-2.png\" height=\"583\" width=\"817\" alt=\"\"  sizes=\"(min-width: 1500px) calc(7.87vw + 610px), (min-width: 900px) calc(44.48vw + 68px), (min-width: 780px) calc(8vw + 604px), calc(84.35vw + 23px)\" srcset=\"https://roger.rogverse.fyi/media/posts/4/responsive/image-2-xs.webp 300w ,https://roger.rogverse.fyi/media/posts/4/responsive/image-2-sm.webp 480w ,https://roger.rogverse.fyi/media/posts/4/responsive/image-2-md.webp 768w\">\n      \n    </figure>\n\n  <p>\n    Crystalwire is a command-line tool that monitors network bandwidth usage for each running process in real-time. It utilizes the psutil library for gathering system information and displays the data in a user-friendly format.<br>\n  </p>\n\n    <h2 id=\"installation\">\n      Installation\n    </h2>\n\n  <p>\n    1. Clone the project\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>gh repo clone rpfilomeno/crystalwire</code></pre>\n\n  <p>\n    2. install 'crystalwire' the dependencies\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>pip install -r requirements.txt</code></pre>\n\n    <h2 id=\"usage\">\n      Usage\n    </h2>\n\n  <p>\n    Once installed, you can run 'crystalwire' from the command line:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>python -m crystalwire.main</code></pre>\n\n  <p>\n    \n  </p>",
            "image": "https://roger.rogverse.fyi/media/posts/4/Python-logo-notext.svg.png",
            "author": {
                "name": "Roger Filomeno"
            },
            "tags": [
                   "Python",
                   "Network",
                   "Monitoring",
                   "Blog"
            ],
            "date_published": "2025-03-22T22:54:21+08:00",
            "date_modified": "2025-03-23T01:27:26+08:00"
        },
        {
            "id": "https://roger.rogverse.fyi/monitoring-kamailio-and-asterisk-with-aws-cloudwatch.html",
            "url": "https://roger.rogverse.fyi/monitoring-kamailio-and-asterisk-with-aws-cloudwatch.html",
            "title": "Monitoring Kamailio and Asterisk with AWS CloudWatch",
            "summary": "Today I'm announcing the release to my new project VOIP Statistics to AWS CloudWatch (voip-mon-aws-cloudwatch), it is a monitoring script for Kamailio and Asterisk for AWS CloudWatch written in PHP. This works similarly to AWS CloudWatch Monitoring Script (Linux). Requirements Installation 1. Git clone to&hellip;",
            "content_html": "\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://roger.rogverse.fyi/media/posts/3/voip-aws-mon-2.jpg\" height=\"647\" width=\"1373\" alt=\"\"  sizes=\"(min-width: 1500px) calc(7.87vw + 610px), (min-width: 900px) calc(44.48vw + 68px), (min-width: 780px) calc(8vw + 604px), calc(84.35vw + 23px)\" srcset=\"https://roger.rogverse.fyi/media/posts/3/responsive/voip-aws-mon-2-xs.webp 300w ,https://roger.rogverse.fyi/media/posts/3/responsive/voip-aws-mon-2-sm.webp 480w ,https://roger.rogverse.fyi/media/posts/3/responsive/voip-aws-mon-2-md.webp 768w\">\n      \n    </figure>\n\n  <p>\n    Today I'm announcing the release to my new project <a href=\"https://github.com/rpfilomeno/voip-mon-aws-cloudwatch\">VOIP Statistics to AWS CloudWatch (voip-mon-aws-cloudwatch)</a>, it is a monitoring script for Kamailio and Asterisk for AWS CloudWatch written in PHP. <br>\n  </p>\n\n  <p>\n    This works similarly to AWS CloudWatch Monitoring Script (Linux).<br><br>Requirements<br>\n  </p>\n\n  <ul>\n    <li>PHP 5.5 and above</li><li>Composer</li><li>Asterisk</li><li>Kamailio</li>\n  </ul>\n\n  <p>\n    Installation<br><br>1. Git clone to any Linux instance with Kamailio or Asterisk installed,<br><br>for example to ~/home/ec2-user/ using&nbsp;\n  </p>\n<pre class=\"line-numbers  language-html\"><code>git clone https://github.com/rpfilomeno/voip-mon-aws-cloudwatch.git</code></pre>\n\n  <p>\n    2. Go to the project's root directory by\n  </p>\n<pre class=\"line-numbers  language-html\"><code>cd ./voip-mon-aws-cloudwatch/</code></pre>\n\n  <p>\n    \n  </p>\n\n  <p>\n    3. Make the mon-put-instance-data.php executable\n  </p>\n<pre class=\"line-numbers  language-html\"><code>sudo chmod +x mon-put-instance-data.php</code></pre>\n\n  <p>\n    4. Install Composer\n  </p>\n<pre class=\"line-numbers  language-html\"><code>curl -sS https://getcomposer.org/installer | php</code></pre>\n\n  <p>\n    5. Install the dependencies by\n  </p>\n<pre class=\"line-numbers  language-html\"><code>php composer.phar update</code></pre>\n\n  <p>\n    6. Create your AWS&nbsp;<a href=\"http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/credentials.html#credential-profiles\" target=\"_blank\">credentials file</a>\n  </p>\n\n    <h2 id=\"monitoring-kamailio\">\n      Monitoring Kamailio\n    </h2>\n\n  <p>\n    1. Test the script for monitoring Kamailio with\n  </p>\n<div>./mon-put-instance-data.php stats --t kamailio</div>\n\n  <p>\n    2. Install to Crontab with\n  </p>\n<div>crontab -e\n*/5 * * * * php /home/ec2-user/voip-mon-aws-cloudwatch/mon-put-instance-data.php stats --s kamailio</div>\n\n    <h2 id=\"monitoring-asterisk\">\n      Monitoring Asterisk\n    </h2>\n\n  <p>\n    1. Test the script for monitoring Kamailio with\n  </p>\n<div>./mon-put-instance-data.php stats --t asterisk\n</div>\n\n  <p>\n    2. Install to Crontab with\n  </p>\n<div>crontab -e\n*/5 * * * * php /home/ec2-user/voip-mon-aws-cloudwatch/mon-put-instance-data.php stats --s asterisk</div>",
            "image": "https://roger.rogverse.fyi/media/posts/3/images-1.png",
            "author": {
                "name": "Roger Filomeno"
            },
            "tags": [
                   "VOIP",
                   "Blog",
                   "AWS"
            ],
            "date_published": "2025-03-22T22:42:13+08:00",
            "date_modified": "2025-03-22T23:18:07+08:00"
        },
        {
            "id": "https://roger.rogverse.fyi/testing-kamailio-load-balancer-with-sipp.html",
            "url": "https://roger.rogverse.fyi/testing-kamailio-load-balancer-with-sipp.html",
            "title": "Testing Kamailio load balancer with SIPp",
            "summary": "Here are the steps to test Kamailio under load. First of all lets describe our network setup: The a user from extension 300X registered to Asterisk 1 initiates a call to an extension 400X registered at Asterisk 2. Kamailio is registered as a trunk to&hellip;",
            "content_html": "<p>Here are the steps to test Kamailio under load.</p><p>First of all lets describe our network setup:</p><script src=\"https://gist.github.com/rpfilomeno/d46493eefaf70d6838c157305ab9778a.js\"></script>\n\n<p>The a user from extension <em>300X</em> registered to <em>Asterisk 1</em> initiates a call to an extension <em>400X</em> registered at <em>Asterisk 2</em>. <em>Kamailio</em> is registered as a <em>trunk</em> to both Asterisk 1 &amp; 2; which intercepts the call which load balances it to either <em>Asterisk X or Y</em> where they do some <em>fancy</em> pre-processing to current call before its received by the callee.</p><p>Now for our testing purposes, we needed to remove the effect on performance by Asterisk 1 &amp; 2 so we installed SIPp on another host which generates calls and receives them.</p><h3 id=\"installation-and-execution-steps\">Installation and Execution Steps</h3>\n<ol>\n<li>Download and Modify SIPp to auto respond always and include OPTIONS packet as well (-aa broken?), edit src/call.cpp:</li>\n</ol>\n<pre><code class=\"language-cpp\">call::T_AutoMode call::checkAutomaticResponseMode(char * P_recv)\n{\n    if (strcmp(P_recv, &quot;BYE&quot;)==0) {\n        return E_AM_UNEXP_BYE;\n    } else if (strcmp(P_recv, &quot;CANCEL&quot;) == 0) {\n        return E_AM_UNEXP_CANCEL;\n    } else if (strcmp(P_recv, &quot;PING&quot;) == 0) {\n        return E_AM_PING;\n    } else if ((strcmp(P_recv, &quot;INFO&quot;) == 0) || (strcmp(P_recv, &quot;NOTIFY&quot;) == 0) || (strcmp(P_recv, &quot;UPDATE&quot;) == 0) || (strcmp(P_recv, &quot;OPTIONS&quot;) == 0)\n               ) {\n        return E_AM_AA;\n    } else {\n        return E_AM_DEFAULT;\n    }\n}\n</code></pre>\n<p>Compile <em>sipp-3.3.990</em> with <a href=\"http://sipp.sourceforge.net/doc/reference.html#Installing+SIPp\">RTP Support</a>.</p><p>To run the test, from SIPp Box: </p><pre><code class=\"language-bash\"># sipp 10.254.1.30 -i 10.254.1.40 -sf uac.xml -aa -inf accounts.csv -l 10000 -r 1 -rp 1000 -trace_msg -trace_err -trace_stat\n</code></pre>\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Parameter</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10.254.1.30</td>\n      <td>target Kamailio's IP on the LAN A side (see network diagram)</td>\n    </tr>\n    <tr>\n      <td>-i 10.254.1.40</td>\n      <td>make sure to bind SIPp on this IP especially if we are using IP Authentication on Kamailio</td>\n    </tr>\n    <tr>\n      <td>-sf <a href=\"https://gist.github.com/rpfilomeno/7445a628a3cbc0ceaaf8e9afe182578b#file-uac-xml\">uac.xml</a></td>\n      <td>use this scenario file that generates calls.</td>\n    </tr>\n    <tr>\n      <td>-inf <a href=\"https://gist.github.com/rpfilomeno/8673ee9dc7355274dfd98d187bbde925#file-accounts-csv\">accounts.csv</a></td>\n      <td>use this input CSV file, this is where the <em>[field0]</em>,<em>[field1]</em>,<em>[field2]</em> and <em>[field3]</em> values are derived in uac.xml. <br>Edit this file accordingly in format: \n        <em>\n        CallID;Kamailio LAN A IP;[authentication];Extension on Asterisk 2;Asterisk 2 LAN B IP;\n        </em>\n    </td>\n    </tr>\n    <tr>\n      <td>-l 10000</td>\n      <td>run 1000 calls.</td>\n    </tr>\n    <tr>\n      <td>-r 1 -rp 1000</td>\n      <td>make one call per 1000ms (1 secs)</td>\n    </tr>\n    <tr>\n      <td>-trace_msg</td>\n      <td>log all messages to a file (filename auto generated)</td>\n    </tr>\n    <tr>\n      <td>-trace_err</td>\n      <td>log all errors to a separate file (filename auto generated)</td>\n    </tr>\n    <tr>\n      <td>-trace_stat</td>\n      <td>generate a CSV file with statistics which is good for making graphs (default 1 minute interval) </td>\n    </tr>\n  </tbody>\n</table>\n\n\n<p>Make sure to edit the accounts.csv, change 10.254.1.30 and 10.254.7.31 accordingly.</p><p>Make sure to edit the uac.xml, change Route:</p><pre><code>&lt;sip:10.254.1.30;r2=on;lr=on;nat=yes&gt;,&lt;sip:10.254.3.30;r2=on;lr=on;nat=yes&gt;```\naccordingly since sipp-3.3.990 can&#39;t reliably generate this header so we had to hard code this for now. \n\nYou may run a SIPp on Asterisk 2 box to test higher concurrent calls (eg: testing more than 200 concurrent calls).\n\nLets shutdown Asterisk 1 &amp; 2: \n```bash\n# asterisk -rx &quot;core stop now&quot;\n</code></pre>\n<p>To run a server listening to incoming calls (server mode), run:</p><pre><code class=\"language-bash\"># sipp 10.254.7.30 -i 10.254.7.31 -sf uas.xml -aa -trace_msg -trace_err -trace_stat\n</code></pre>\n<p>Makes sure to edit the <a href=\"https://gist.github.com/rpfilomeno/5827e6ecf5863f74f53d41b1e15fa707#file-uas-xml\">uas.xml</a> to include the IP routes.</p><p>Now lets see how effective is Kamailio in this setup, here are the results I had:</p><table class=\"table\">\n  <thead>\n    <tr>\n      <th>Test Name</th>\n      <th>Concurrent Calls</th>\n      <th>Success</th>\n      <th>Failed</th>\n      <th>Dead Calls</th>\n      <th>Retransmissions</th>\n      <th>Average Response Time</th>\n      <th>Average Call Rate Per Seconds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Test1</td>\n      <td>200</td>\n      <td>1000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2.52747</td>\n      <td>03.615000</td>\n    </tr>\n    <tr>\n      <td>Test2</td>\n      <td>300</td>\n      <td>998</td>\n      <td>2</td>\n      <td>5</td>\n      <td>252</td>\n      <td>3.15839</td>\n      <td>04.550000</td>\n    </tr>\n    <tr>\n      <td>Test3</td>\n      <td>400</td>\n      <td>993</td>\n      <td>7</td>\n      <td>12</td>\n      <td>1355</td>\n      <td>3.61512</td>\n      <td>13.049000</td>\n    </tr>\n    <tr>\n      <td>Test4</td>\n      <td>600</td>\n      <td>831</td>\n      <td>169</td>\n      <td>127</td>\n      <td>3554</td>\n      <td>4.05337</td>\n      <td>13.04900</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>We stop at <em>Test 4</em> seeing Failed Calls spiked up at 169 calls, this was significant from our base capacity of 50 concurrent calls already.</p><p>Many thanks to <a href=\"http://saevolgo.blogspot.com/\">Gohar Ahmed</a> for helping me figuring most of the bugs.</p>",
            "image": "https://roger.rogverse.fyi/media/posts/2/5348744.png",
            "author": {
                "name": "Roger Filomeno"
            },
            "tags": [
                   "VOIP",
                   "Testing",
                   "Blog"
            ],
            "date_published": "2025-03-22T22:11:12+08:00",
            "date_modified": "2025-03-22T23:18:22+08:00"
        }
    ]
}
